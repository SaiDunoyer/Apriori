{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "controllo documentale training.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaiDunoyer/Apriori/blob/master/Doc_classification_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aqn2wsO4Rzo"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7OBoVzk4eTM",
        "outputId": "b104e50c-c2ea-4c7c-91a4-212ede17fa91"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCTl_J-N51GY",
        "outputId": "78a325d7-6a15-4131-adba-d326a82283af"
      },
      "source": [
        "# avg w = 2481 average h = 3509 # 1200, 1750\n",
        "img_width, img_height = 372, 532\n",
        "\n",
        "base_path = '/content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4'\n",
        "train_data_dir = os.path.join(base_path, 'train')\n",
        "labels = [label for label in os.listdir(train_data_dir)]\n",
        "validation_data_dir = os.path.join(base_path, 'test')\n",
        "nb_train_samples = sum([len(os.listdir(os.path.join(train_data_dir, label))) for label in labels])\n",
        "nb_validation_samples = sum([len(os.listdir(os.path.join(validation_data_dir, label))) for label in labels])\n",
        "\n",
        "for label in labels:\n",
        "    for png_file in os.listdir(os.path.join(train_data_dir, label)):\n",
        "        X = image.load_img(os.path.join(train_data_dir, label, png_file), target_size=(img_width, img_height))\n",
        "        X = image.img_to_array(X)\n",
        "        X = preprocess_input(X)\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 10\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    input_shape = (3, img_width, img_height)\n",
        "else:\n",
        "    input_shape = (img_width, img_height, 3)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(12))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=Adam(lr=0.001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_data_dir,\n",
        "                                                    target_size=(img_width, img_height),\n",
        "                                                    batch_size=batch_size, class_mode='binary')\n",
        "\n",
        "print('train', train_generator.class_indices)\n",
        "weights = {v: 1 - len(os.listdir(os.path.join(train_data_dir, k))) / nb_train_samples for k,v in train_generator.class_indices.items()}\n",
        "print('weights', weights)\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size, class_mode='binary')\n",
        "\n",
        "print('test', validation_generator.class_indices)\n",
        "\n",
        "\n",
        "\n",
        "checkpoint = ModelCheckpoint(os.path.join(base_path, 'best_model_xs_v4.h5'), monitor='loss', verbose=1,\n",
        "                             save_best_only=True, mode='auto', save_freq=1)\n",
        "\n",
        "model.fit(train_generator,\n",
        "                    steps_per_epoch=nb_train_samples // batch_size,\n",
        "                    epochs=epochs, validation_data=validation_generator,\n",
        "                    validation_steps=nb_validation_samples // batch_size,\n",
        "                    class_weight=weights,\n",
        "                    callbacks=[checkpoint])\n",
        "\n",
        "model.save_weights(os.path.join(base_path, 'model_saved_xs_v4.h5'))\n",
        "\n",
        "with open(os.path.join(base_path, 'model_saved_xs_v4.json'), 'w') as f:\n",
        "    f.write(model.to_json())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 1362 images belonging to 9 classes.\n",
            "train {'bil': 0, 'ce_sp': 1, 'doc_id': 2, 'irap': 3, 'other': 4, 'pf': 5, 'ric_cc': 6, 'ric_unico': 7, 'sp': 8}\n",
            "weights {0: 0.9904552129221733, 1: 0.9588839941262849, 2: 0.711453744493392, 3: 0.9654919236417033, 4: 0.5778267254038179, 5: 0.9405286343612335, 6: 0.9243759177679882, 7: 0.9691629955947136, 8: 0.9618208516886931}\n",
            "Found 243 images belonging to 9 classes.\n",
            "test {'bil': 0, 'ce_sp': 1, 'doc_id': 2, 'irap': 3, 'other': 4, 'pf': 5, 'ric_cc': 6, 'ric_unico': 7, 'sp': 8}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
            "Epoch 1/10\n",
            "  1/136 [..............................] - ETA: 48:28 - loss: 44.8923 - accuracy: 0.0000e+00\n",
            "Epoch 00001: loss improved from inf to 44.89225, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "  2/136 [..............................] - ETA: 10:13 - loss: 415.8456 - accuracy: 0.0750   \n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            "  3/136 [..............................] - ETA: 9:17 - loss: 585.0969 - accuracy: 0.1167 \n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            "  4/136 [..............................] - ETA: 8:49 - loss: 649.7261 - accuracy: 0.1625\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            "  5/136 [>.............................] - ETA: 8:32 - loss: 663.4829 - accuracy: 0.1860\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            "  6/136 [>.............................] - ETA: 8:18 - loss: 656.2594 - accuracy: 0.1967\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            "  7/136 [>.............................] - ETA: 8:05 - loss: 641.2503 - accuracy: 0.2053\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            "  8/136 [>.............................] - ETA: 7:55 - loss: 622.2005 - accuracy: 0.2140\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            "  9/136 [>.............................] - ETA: 7:46 - loss: 602.7466 - accuracy: 0.2223\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 10/136 [=>............................] - ETA: 7:37 - loss: 583.3180 - accuracy: 0.2271\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 11/136 [=>............................] - ETA: 7:29 - loss: 564.2018 - accuracy: 0.2337\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 12/136 [=>............................] - ETA: 7:26 - loss: 545.7465 - accuracy: 0.2386\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 13/136 [=>............................] - ETA: 7:20 - loss: 528.1492 - accuracy: 0.2445\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 14/136 [==>...........................] - ETA: 7:16 - loss: 511.4563 - accuracy: 0.2510\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 15/136 [==>...........................] - ETA: 7:10 - loss: 495.7076 - accuracy: 0.2578\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 16/136 [==>...........................] - ETA: 7:06 - loss: 480.8576 - accuracy: 0.2655\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 17/136 [==>...........................] - ETA: 7:02 - loss: 466.8660 - accuracy: 0.2724\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 18/136 [==>...........................] - ETA: 6:58 - loss: 453.7047 - accuracy: 0.2789\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 19/136 [===>..........................] - ETA: 6:54 - loss: 441.2966 - accuracy: 0.2844\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 20/136 [===>..........................] - ETA: 6:49 - loss: 429.5880 - accuracy: 0.2897\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 21/136 [===>..........................] - ETA: 6:45 - loss: 418.5281 - accuracy: 0.2940\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 22/136 [===>..........................] - ETA: 6:40 - loss: 408.0690 - accuracy: 0.2978\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 23/136 [====>.........................] - ETA: 6:36 - loss: 398.1666 - accuracy: 0.3011\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 24/136 [====>.........................] - ETA: 6:31 - loss: 388.7794 - accuracy: 0.3039\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 25/136 [====>.........................] - ETA: 6:27 - loss: 379.8686 - accuracy: 0.3064\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 26/136 [====>.........................] - ETA: 6:22 - loss: 371.3998 - accuracy: 0.3088\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 27/136 [====>.........................] - ETA: 6:19 - loss: 363.3407 - accuracy: 0.3113\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 28/136 [=====>........................] - ETA: 6:16 - loss: 355.6629 - accuracy: 0.3137\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 29/136 [=====>........................] - ETA: 6:11 - loss: 348.3398 - accuracy: 0.3159\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 30/136 [=====>........................] - ETA: 6:08 - loss: 341.3469 - accuracy: 0.3184\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 31/136 [=====>........................] - ETA: 6:05 - loss: 334.6627 - accuracy: 0.3206\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 32/136 [======>.......................] - ETA: 6:00 - loss: 328.2678 - accuracy: 0.3226\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 33/136 [======>.......................] - ETA: 5:56 - loss: 322.1427 - accuracy: 0.3246\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 34/136 [======>.......................] - ETA: 5:52 - loss: 316.2699 - accuracy: 0.3267\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 35/136 [======>.......................] - ETA: 5:48 - loss: 310.6340 - accuracy: 0.3286\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 36/136 [======>.......................] - ETA: 5:45 - loss: 305.2202 - accuracy: 0.3304\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 37/136 [=======>......................] - ETA: 5:42 - loss: 300.0155 - accuracy: 0.3321\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 38/136 [=======>......................] - ETA: 5:38 - loss: 295.0082 - accuracy: 0.3337\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 39/136 [=======>......................] - ETA: 5:34 - loss: 290.1866 - accuracy: 0.3353\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 40/136 [=======>......................] - ETA: 5:31 - loss: 285.5399 - accuracy: 0.3370\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 41/136 [========>.....................] - ETA: 5:27 - loss: 281.0584 - accuracy: 0.3387\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 42/136 [========>.....................] - ETA: 5:23 - loss: 276.7334 - accuracy: 0.3404\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 43/136 [========>.....................] - ETA: 5:19 - loss: 272.5565 - accuracy: 0.3422\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 44/136 [========>.....................] - ETA: 5:16 - loss: 268.5201 - accuracy: 0.3440\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 45/136 [========>.....................] - ETA: 5:12 - loss: 264.6170 - accuracy: 0.3457\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 46/136 [=========>....................] - ETA: 5:09 - loss: 260.8403 - accuracy: 0.3475\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 47/136 [=========>....................] - ETA: 5:05 - loss: 257.1840 - accuracy: 0.3494\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 48/136 [=========>....................] - ETA: 5:01 - loss: 253.6420 - accuracy: 0.3513\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 49/136 [=========>....................] - ETA: 4:57 - loss: 250.2086 - accuracy: 0.3533\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 50/136 [==========>...................] - ETA: 4:54 - loss: 246.8789 - accuracy: 0.3554\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 51/136 [==========>...................] - ETA: 4:50 - loss: 243.6482 - accuracy: 0.3573\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 52/136 [==========>...................] - ETA: 4:47 - loss: 240.5119 - accuracy: 0.3592\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 53/136 [==========>...................] - ETA: 4:44 - loss: 237.4656 - accuracy: 0.3610\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 54/136 [==========>...................] - ETA: 4:40 - loss: 234.5055 - accuracy: 0.3628\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 55/136 [===========>..................] - ETA: 4:37 - loss: 231.6278 - accuracy: 0.3646\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 56/136 [===========>..................] - ETA: 4:33 - loss: 228.8294 - accuracy: 0.3663\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 57/136 [===========>..................] - ETA: 4:30 - loss: 226.1062 - accuracy: 0.3681\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 58/136 [===========>..................] - ETA: 4:26 - loss: 223.4569 - accuracy: 0.3698\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 59/136 [============>.................] - ETA: 4:23 - loss: 220.8769 - accuracy: 0.3715\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 60/136 [============>.................] - ETA: 4:19 - loss: 218.3635 - accuracy: 0.3732\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 61/136 [============>.................] - ETA: 4:16 - loss: 215.9140 - accuracy: 0.3749\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 62/136 [============>.................] - ETA: 4:12 - loss: 213.5260 - accuracy: 0.3767\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 63/136 [============>.................] - ETA: 4:09 - loss: 211.1970 - accuracy: 0.3783\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 64/136 [=============>................] - ETA: 4:05 - loss: 208.9249 - accuracy: 0.3800\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 65/136 [=============>................] - ETA: 4:02 - loss: 206.7074 - accuracy: 0.3817\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 66/136 [=============>................] - ETA: 3:58 - loss: 204.5427 - accuracy: 0.3833\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 67/136 [=============>................] - ETA: 3:55 - loss: 202.4287 - accuracy: 0.3849\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 68/136 [==============>...............] - ETA: 3:51 - loss: 200.3635 - accuracy: 0.3864\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 69/136 [==============>...............] - ETA: 3:48 - loss: 198.3453 - accuracy: 0.3880\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 70/136 [==============>...............] - ETA: 3:44 - loss: 196.3726 - accuracy: 0.3896\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 71/136 [==============>...............] - ETA: 3:41 - loss: 194.4439 - accuracy: 0.3911\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 72/136 [==============>...............] - ETA: 3:37 - loss: 192.5576 - accuracy: 0.3925\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 73/136 [===============>..............] - ETA: 3:34 - loss: 190.7121 - accuracy: 0.3940\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 74/136 [===============>..............] - ETA: 3:31 - loss: 188.9062 - accuracy: 0.3953\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 75/136 [===============>..............] - ETA: 3:27 - loss: 187.1386 - accuracy: 0.3966\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 76/136 [===============>..............] - ETA: 3:24 - loss: 185.4079 - accuracy: 0.3979\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 77/136 [===============>..............] - ETA: 3:20 - loss: 183.7130 - accuracy: 0.3993\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 78/136 [================>.............] - ETA: 3:17 - loss: 182.0527 - accuracy: 0.4006\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 79/136 [================>.............] - ETA: 3:13 - loss: 180.4259 - accuracy: 0.4020\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 80/136 [================>.............] - ETA: 3:10 - loss: 178.8317 - accuracy: 0.4032\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 81/136 [================>.............] - ETA: 3:07 - loss: 177.2689 - accuracy: 0.4045\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 82/136 [=================>............] - ETA: 3:03 - loss: 175.7367 - accuracy: 0.4058\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 83/136 [=================>............] - ETA: 3:00 - loss: 174.2341 - accuracy: 0.4070\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 84/136 [=================>............] - ETA: 2:56 - loss: 172.7602 - accuracy: 0.4082\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 85/136 [=================>............] - ETA: 2:53 - loss: 171.3141 - accuracy: 0.4094\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 86/136 [=================>............] - ETA: 2:50 - loss: 169.8951 - accuracy: 0.4107\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 87/136 [==================>...........] - ETA: 2:46 - loss: 168.5023 - accuracy: 0.4119\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 88/136 [==================>...........] - ETA: 2:43 - loss: 167.1350 - accuracy: 0.4132\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 89/136 [==================>...........] - ETA: 2:39 - loss: 165.7925 - accuracy: 0.4144\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 90/136 [==================>...........] - ETA: 2:36 - loss: 164.4742 - accuracy: 0.4156\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 91/136 [===================>..........] - ETA: 2:33 - loss: 163.1793 - accuracy: 0.4168\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 92/136 [===================>..........] - ETA: 2:29 - loss: 161.9072 - accuracy: 0.4180\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 93/136 [===================>..........] - ETA: 2:26 - loss: 160.6573 - accuracy: 0.4192\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 94/136 [===================>..........] - ETA: 2:22 - loss: 159.4289 - accuracy: 0.4204\n",
            "Epoch 00001: loss did not improve from 44.89225\n",
            " 95/136 [===================>..........] - ETA: 2:19 - loss: 158.2214 - accuracy: 0.4216\n",
            "Epoch 00001: loss improved from 44.89225 to 44.72162, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            " 96/136 [====================>.........] - ETA: 2:16 - loss: 157.0344 - accuracy: 0.4228\n",
            "Epoch 00001: loss improved from 44.72162 to 44.26794, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            " 97/136 [====================>.........] - ETA: 2:13 - loss: 155.8673 - accuracy: 0.4240\n",
            "Epoch 00001: loss improved from 44.26794 to 43.82821, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            " 98/136 [====================>.........] - ETA: 2:09 - loss: 154.7196 - accuracy: 0.4252\n",
            "Epoch 00001: loss improved from 43.82821 to 43.39086, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            " 99/136 [====================>.........] - ETA: 2:06 - loss: 153.5908 - accuracy: 0.4263\n",
            "Epoch 00001: loss improved from 43.39086 to 42.97136, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "100/136 [=====================>........] - ETA: 2:03 - loss: 152.4805 - accuracy: 0.4274\n",
            "Epoch 00001: loss improved from 42.97136 to 42.55977, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "101/136 [=====================>........] - ETA: 2:00 - loss: 151.3881 - accuracy: 0.4285\n",
            "Epoch 00001: loss improved from 42.55977 to 42.14875, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "102/136 [=====================>........] - ETA: 1:56 - loss: 150.3132 - accuracy: 0.4296\n",
            "Epoch 00001: loss improved from 42.14875 to 41.74754, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "103/136 [=====================>........] - ETA: 1:53 - loss: 149.2554 - accuracy: 0.4307\n",
            "Epoch 00001: loss improved from 41.74754 to 41.35574, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "104/136 [=====================>........] - ETA: 1:50 - loss: 148.2141 - accuracy: 0.4318\n",
            "Epoch 00001: loss improved from 41.35574 to 40.96444, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "105/136 [======================>.......] - ETA: 1:46 - loss: 147.1891 - accuracy: 0.4329\n",
            "Epoch 00001: loss improved from 40.96444 to 40.58771, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "106/136 [======================>.......] - ETA: 1:42 - loss: 146.1827 - accuracy: 0.4339\n",
            "Epoch 00001: loss improved from 40.58771 to 40.51339, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "107/136 [======================>.......] - ETA: 1:39 - loss: 145.1917 - accuracy: 0.4350\n",
            "Epoch 00001: loss improved from 40.51339 to 40.14086, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "108/136 [======================>.......] - ETA: 1:36 - loss: 144.2156 - accuracy: 0.4361\n",
            "Epoch 00001: loss improved from 40.14086 to 39.77637, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "109/136 [=======================>......] - ETA: 1:33 - loss: 143.2542 - accuracy: 0.4371\n",
            "Epoch 00001: loss improved from 39.77637 to 39.41650, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "110/136 [=======================>......] - ETA: 1:29 - loss: 142.3070 - accuracy: 0.4382\n",
            "Epoch 00001: loss improved from 39.41650 to 39.06471, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "111/136 [=======================>......] - ETA: 1:26 - loss: 141.3738 - accuracy: 0.4392\n",
            "Epoch 00001: loss improved from 39.06471 to 38.72544, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "112/136 [=======================>......] - ETA: 1:22 - loss: 140.4544 - accuracy: 0.4401\n",
            "Epoch 00001: loss improved from 38.72544 to 38.39810, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "113/136 [=======================>......] - ETA: 1:19 - loss: 139.5483 - accuracy: 0.4411\n",
            "Epoch 00001: loss improved from 38.39810 to 38.06510, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "114/136 [========================>.....] - ETA: 1:16 - loss: 138.6552 - accuracy: 0.4421\n",
            "Epoch 00001: loss improved from 38.06510 to 37.73794, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "115/136 [========================>.....] - ETA: 1:12 - loss: 137.7749 - accuracy: 0.4430\n",
            "Epoch 00001: loss improved from 37.73794 to 37.41857, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "116/136 [========================>.....] - ETA: 1:09 - loss: 136.9071 - accuracy: 0.4440\n",
            "Epoch 00001: loss improved from 37.41857 to 37.10494, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "117/136 [========================>.....] - ETA: 1:06 - loss: 136.0514 - accuracy: 0.4449\n",
            "Epoch 00001: loss improved from 37.10494 to 36.79808, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "118/136 [=========================>....] - ETA: 1:03 - loss: 135.2077 - accuracy: 0.4458\n",
            "Epoch 00001: loss improved from 36.79808 to 36.49271, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "119/136 [=========================>....] - ETA: 59s - loss: 134.3758 - accuracy: 0.4467 \n",
            "Epoch 00001: loss improved from 36.49271 to 36.20539, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "120/136 [=========================>....] - ETA: 56s - loss: 133.5552 - accuracy: 0.4476\n",
            "Epoch 00001: loss improved from 36.20539 to 35.91077, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "121/136 [=========================>....] - ETA: 53s - loss: 132.7459 - accuracy: 0.4484\n",
            "Epoch 00001: loss improved from 35.91077 to 35.62536, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "122/136 [=========================>....] - ETA: 49s - loss: 131.9475 - accuracy: 0.4493\n",
            "Epoch 00001: loss improved from 35.62536 to 35.34000, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "123/136 [==========================>...] - ETA: 46s - loss: 131.1598 - accuracy: 0.4501\n",
            "Epoch 00001: loss improved from 35.34000 to 35.06607, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "124/136 [==========================>...] - ETA: 42s - loss: 130.3827 - accuracy: 0.4509\n",
            "Epoch 00001: loss improved from 35.06607 to 34.79297, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "125/136 [==========================>...] - ETA: 39s - loss: 129.6158 - accuracy: 0.4517\n",
            "Epoch 00001: loss improved from 34.79297 to 34.52345, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "126/136 [==========================>...] - ETA: 35s - loss: 128.8590 - accuracy: 0.4525\n",
            "Epoch 00001: loss improved from 34.52345 to 34.25879, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "127/136 [===========================>..] - ETA: 32s - loss: 128.1120 - accuracy: 0.4533\n",
            "Epoch 00001: loss improved from 34.25879 to 33.99513, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "128/136 [===========================>..] - ETA: 28s - loss: 127.3747 - accuracy: 0.4542\n",
            "Epoch 00001: loss improved from 33.99513 to 33.73499, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "129/136 [===========================>..] - ETA: 24s - loss: 126.6468 - accuracy: 0.4550\n",
            "Epoch 00001: loss improved from 33.73499 to 33.47846, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "130/136 [===========================>..] - ETA: 21s - loss: 125.9282 - accuracy: 0.4558\n",
            "Epoch 00001: loss improved from 33.47846 to 33.22884, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "131/136 [===========================>..] - ETA: 17s - loss: 125.2188 - accuracy: 0.4566\n",
            "Epoch 00001: loss improved from 33.22884 to 32.98660, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "132/136 [============================>.] - ETA: 14s - loss: 124.5182 - accuracy: 0.4574\n",
            "Epoch 00001: loss improved from 32.98660 to 32.74084, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "133/136 [============================>.] - ETA: 10s - loss: 123.8263 - accuracy: 0.4582\n",
            "Epoch 00001: loss improved from 32.74084 to 32.50160, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "134/136 [============================>.] - ETA: 7s - loss: 123.1431 - accuracy: 0.4589 \n",
            "Epoch 00001: loss improved from 32.50160 to 32.26927, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "135/136 [============================>.] - ETA: 3s - loss: 122.4682 - accuracy: 0.4597\n",
            "Epoch 00001: loss improved from 32.26927 to 32.03769, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "136/136 [==============================] - ETA: 0s - loss: 121.8016 - accuracy: 0.4605\n",
            "Epoch 00001: loss improved from 32.03769 to 31.81148, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "136/136 [==============================] - 629s 4s/step - loss: 121.1447 - accuracy: 0.4612 - val_loss: 1.7871 - val_accuracy: 0.6792\n",
            "Epoch 2/10\n",
            "  1/136 [..............................] - ETA: 11:33 - loss: 1.1743 - accuracy: 0.6000\n",
            "Epoch 00002: loss improved from 31.81148 to 1.17429, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "  2/136 [..............................] - ETA: 12:39 - loss: 1.3038 - accuracy: 0.5750\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            "  3/136 [..............................] - ETA: 10:38 - loss: 1.3558 - accuracy: 0.5722\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            "  4/136 [..............................] - ETA: 9:46 - loss: 1.3572 - accuracy: 0.5854 \n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            "  5/136 [>.............................] - ETA: 9:12 - loss: 1.3625 - accuracy: 0.5963\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            "  6/136 [>.............................] - ETA: 8:47 - loss: 1.3565 - accuracy: 0.6081\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            "  7/136 [>.............................] - ETA: 8:29 - loss: 1.3543 - accuracy: 0.6151\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            "  8/136 [>.............................] - ETA: 8:14 - loss: 1.3524 - accuracy: 0.6194\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            "  9/136 [>.............................] - ETA: 8:00 - loss: 1.3472 - accuracy: 0.6259\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 10/136 [=>............................] - ETA: 7:56 - loss: 1.3419 - accuracy: 0.6323\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 11/136 [=>............................] - ETA: 7:48 - loss: 1.3354 - accuracy: 0.6393\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 12/136 [=>............................] - ETA: 7:40 - loss: 1.3278 - accuracy: 0.6464\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 13/136 [=>............................] - ETA: 7:33 - loss: 1.3210 - accuracy: 0.6523\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 14/136 [==>...........................] - ETA: 7:26 - loss: 1.3136 - accuracy: 0.6578\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 15/136 [==>...........................] - ETA: 7:20 - loss: 1.3072 - accuracy: 0.6628\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 16/136 [==>...........................] - ETA: 7:14 - loss: 1.3033 - accuracy: 0.6663\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 17/136 [==>...........................] - ETA: 7:10 - loss: 1.3014 - accuracy: 0.6680\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 18/136 [==>...........................] - ETA: 7:05 - loss: 1.3007 - accuracy: 0.6688\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 19/136 [===>..........................] - ETA: 6:59 - loss: 1.3005 - accuracy: 0.6691\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 20/136 [===>..........................] - ETA: 6:54 - loss: 1.3003 - accuracy: 0.6696\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 21/136 [===>..........................] - ETA: 6:51 - loss: 1.2999 - accuracy: 0.6699\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 22/136 [===>..........................] - ETA: 6:46 - loss: 1.2997 - accuracy: 0.6703\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 23/136 [====>.........................] - ETA: 6:41 - loss: 1.2995 - accuracy: 0.6704\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 24/136 [====>.........................] - ETA: 6:38 - loss: 1.2993 - accuracy: 0.6703\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 25/136 [====>.........................] - ETA: 6:34 - loss: 1.2994 - accuracy: 0.6699\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 26/136 [====>.........................] - ETA: 6:28 - loss: 1.2992 - accuracy: 0.6697\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 27/136 [====>.........................] - ETA: 6:25 - loss: 1.2985 - accuracy: 0.6697\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 28/136 [=====>........................] - ETA: 6:22 - loss: 1.2972 - accuracy: 0.6699\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 29/136 [=====>........................] - ETA: 6:18 - loss: 1.2956 - accuracy: 0.6702\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 30/136 [=====>........................] - ETA: 6:14 - loss: 1.2940 - accuracy: 0.6705\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 31/136 [=====>........................] - ETA: 6:07 - loss: 1.2926 - accuracy: 0.6708\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 32/136 [======>.......................] - ETA: 6:03 - loss: 1.2907 - accuracy: 0.6712\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 33/136 [======>.......................] - ETA: 5:59 - loss: 1.2888 - accuracy: 0.6717\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 34/136 [======>.......................] - ETA: 5:47 - loss: 1.2872 - accuracy: 0.6721\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 35/136 [======>.......................] - ETA: 5:45 - loss: 1.2855 - accuracy: 0.6724\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 36/136 [======>.......................] - ETA: 5:42 - loss: 1.2839 - accuracy: 0.6727\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 37/136 [=======>......................] - ETA: 5:38 - loss: 1.2820 - accuracy: 0.6730\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 38/136 [=======>......................] - ETA: 5:35 - loss: 1.2803 - accuracy: 0.6730\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 39/136 [=======>......................] - ETA: 5:31 - loss: 1.2786 - accuracy: 0.6732\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 40/136 [=======>......................] - ETA: 5:28 - loss: 1.2769 - accuracy: 0.6734\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 41/136 [========>.....................] - ETA: 5:25 - loss: 1.2749 - accuracy: 0.6736\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 42/136 [========>.....................] - ETA: 5:21 - loss: 1.2732 - accuracy: 0.6737\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 43/136 [========>.....................] - ETA: 5:18 - loss: 1.2713 - accuracy: 0.6739\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 44/136 [========>.....................] - ETA: 5:14 - loss: 1.2692 - accuracy: 0.6742\n",
            "Epoch 00002: loss did not improve from 1.17429\n",
            " 45/136 [========>.....................] - ETA: 5:11 - loss: 1.2670 - accuracy: 0.6745\n",
            "Epoch 00002: loss improved from 1.17429 to 1.17105, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            " 46/136 [=========>....................] - ETA: 5:12 - loss: 1.2647 - accuracy: 0.6749\n",
            "Epoch 00002: loss improved from 1.17105 to 1.15772, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            " 47/136 [=========>....................] - ETA: 5:09 - loss: 1.2621 - accuracy: 0.6754\n",
            "Epoch 00002: loss improved from 1.15772 to 1.14285, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            " 48/136 [=========>....................] - ETA: 5:06 - loss: 1.2595 - accuracy: 0.6759\n",
            "Epoch 00002: loss improved from 1.14285 to 1.14023, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            " 49/136 [=========>....................] - ETA: 5:04 - loss: 1.2568 - accuracy: 0.6764\n",
            "Epoch 00002: loss improved from 1.14023 to 1.12649, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            " 50/136 [==========>...................] - ETA: 5:02 - loss: 1.2541 - accuracy: 0.6771\n",
            "Epoch 00002: loss improved from 1.12649 to 1.12357, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            " 51/136 [==========>...................] - ETA: 4:59 - loss: 1.2515 - accuracy: 0.6776\n",
            "Epoch 00002: loss improved from 1.12357 to 1.12079, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            " 52/136 [==========>...................] - ETA: 4:56 - loss: 1.2488 - accuracy: 0.6782\n",
            "Epoch 00002: loss improved from 1.12079 to 1.11150, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            " 53/136 [==========>...................] - ETA: 4:53 - loss: 1.2460 - accuracy: 0.6789\n",
            "Epoch 00002: loss improved from 1.11150 to 1.10052, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            " 54/136 [==========>...................] - ETA: 4:51 - loss: 1.2433 - accuracy: 0.6796\n",
            "Epoch 00002: loss improved from 1.10052 to 1.09982, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            " 55/136 [===========>..................] - ETA: 4:48 - loss: 1.2408 - accuracy: 0.6801\n",
            "Epoch 00002: loss did not improve from 1.09982\n",
            " 56/136 [===========>..................] - ETA: 4:44 - loss: 1.2383 - accuracy: 0.6807\n",
            "Epoch 00002: loss did not improve from 1.09982\n",
            " 57/136 [===========>..................] - ETA: 4:41 - loss: 1.2357 - accuracy: 0.6813\n",
            "Epoch 00002: loss improved from 1.09982 to 1.08973, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            " 58/136 [===========>..................] - ETA: 4:38 - loss: 1.2331 - accuracy: 0.6819\n",
            "Epoch 00002: loss improved from 1.08973 to 1.08435, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            " 59/136 [============>.................] - ETA: 4:35 - loss: 1.2304 - accuracy: 0.6826\n",
            "Epoch 00002: loss improved from 1.08435 to 1.07347, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            " 60/136 [============>.................] - ETA: 4:32 - loss: 1.2278 - accuracy: 0.6832\n",
            "Epoch 00002: loss did not improve from 1.07347\n",
            " 61/136 [============>.................] - ETA: 4:28 - loss: 1.2253 - accuracy: 0.6839\n",
            "Epoch 00002: loss did not improve from 1.07347\n",
            " 62/136 [============>.................] - ETA: 4:25 - loss: 1.2228 - accuracy: 0.6845\n",
            "Epoch 00002: loss improved from 1.07347 to 1.06875, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            " 63/136 [============>.................] - ETA: 4:22 - loss: 1.2205 - accuracy: 0.6851\n",
            "Epoch 00002: loss did not improve from 1.06875\n",
            " 64/136 [=============>................] - ETA: 4:18 - loss: 1.2180 - accuracy: 0.6857\n",
            "Epoch 00002: loss improved from 1.06875 to 1.06415, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            " 65/136 [=============>................] - ETA: 4:15 - loss: 1.2157 - accuracy: 0.6862\n",
            "Epoch 00002: loss did not improve from 1.06415\n",
            " 66/136 [=============>................] - ETA: 4:11 - loss: 1.2135 - accuracy: 0.6868\n",
            "Epoch 00002: loss did not improve from 1.06415\n",
            " 67/136 [=============>................] - ETA: 4:08 - loss: 1.2114 - accuracy: 0.6872\n",
            "Epoch 00002: loss did not improve from 1.06415\n",
            " 68/136 [==============>...............] - ETA: 4:04 - loss: 1.2093 - accuracy: 0.6877\n",
            "Epoch 00002: loss did not improve from 1.06415\n",
            " 69/136 [==============>...............] - ETA: 4:00 - loss: 1.2074 - accuracy: 0.6881\n",
            "Epoch 00002: loss did not improve from 1.06415\n",
            " 70/136 [==============>...............] - ETA: 3:56 - loss: 1.2055 - accuracy: 0.6885\n",
            "Epoch 00002: loss did not improve from 1.06415\n",
            " 71/136 [==============>...............] - ETA: 3:52 - loss: 1.2037 - accuracy: 0.6890\n",
            "Epoch 00002: loss did not improve from 1.06415\n",
            " 72/136 [==============>...............] - ETA: 3:49 - loss: 1.2019 - accuracy: 0.6894\n",
            "Epoch 00002: loss did not improve from 1.06415\n",
            " 73/136 [===============>..............] - ETA: 3:45 - loss: 1.2001 - accuracy: 0.6898\n",
            "Epoch 00002: loss did not improve from 1.06415\n",
            " 74/136 [===============>..............] - ETA: 3:42 - loss: 1.1984 - accuracy: 0.6902\n",
            "Epoch 00002: loss did not improve from 1.06415\n",
            " 75/136 [===============>..............] - ETA: 3:38 - loss: 1.1967 - accuracy: 0.6907\n",
            "Epoch 00002: loss did not improve from 1.06415\n",
            " 76/136 [===============>..............] - ETA: 3:34 - loss: 1.1949 - accuracy: 0.6911\n",
            "Epoch 00002: loss did not improve from 1.06415\n",
            " 77/136 [===============>..............] - ETA: 3:30 - loss: 1.1932 - accuracy: 0.6916\n",
            "Epoch 00002: loss improved from 1.06415 to 1.06308, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            " 78/136 [================>.............] - ETA: 3:28 - loss: 1.1916 - accuracy: 0.6920\n",
            "Epoch 00002: loss did not improve from 1.06308\n",
            " 79/136 [================>.............] - ETA: 3:24 - loss: 1.1901 - accuracy: 0.6924\n",
            "Epoch 00002: loss did not improve from 1.06308\n",
            " 80/136 [================>.............] - ETA: 3:21 - loss: 1.1885 - accuracy: 0.6928\n",
            "Epoch 00002: loss did not improve from 1.06308\n",
            " 81/136 [================>.............] - ETA: 3:17 - loss: 1.1870 - accuracy: 0.6932\n",
            "Epoch 00002: loss did not improve from 1.06308\n",
            " 82/136 [=================>............] - ETA: 3:13 - loss: 1.1856 - accuracy: 0.6936\n",
            "Epoch 00002: loss did not improve from 1.06308\n",
            " 83/136 [=================>............] - ETA: 3:10 - loss: 1.1842 - accuracy: 0.6940\n",
            "Epoch 00002: loss did not improve from 1.06308\n",
            " 84/136 [=================>............] - ETA: 3:06 - loss: 1.1828 - accuracy: 0.6943\n",
            "Epoch 00002: loss did not improve from 1.06308\n",
            " 85/136 [=================>............] - ETA: 3:02 - loss: 1.1815 - accuracy: 0.6947\n",
            "Epoch 00002: loss did not improve from 1.06308\n",
            " 86/136 [=================>............] - ETA: 2:59 - loss: 1.1802 - accuracy: 0.6951\n",
            "Epoch 00002: loss did not improve from 1.06308\n",
            " 87/136 [==================>...........] - ETA: 2:55 - loss: 1.1789 - accuracy: 0.6954\n",
            "Epoch 00002: loss did not improve from 1.06308\n",
            " 88/136 [==================>...........] - ETA: 2:51 - loss: 1.1775 - accuracy: 0.6958\n",
            "Epoch 00002: loss improved from 1.06308 to 1.06064, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            " 89/136 [==================>...........] - ETA: 2:49 - loss: 1.1762 - accuracy: 0.6962\n",
            "Epoch 00002: loss improved from 1.06064 to 1.05493, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            " 90/136 [==================>...........] - ETA: 2:45 - loss: 1.1749 - accuracy: 0.6966\n",
            "Epoch 00002: loss did not improve from 1.05493\n",
            " 91/136 [===================>..........] - ETA: 2:42 - loss: 1.1736 - accuracy: 0.6971\n",
            "Epoch 00002: loss did not improve from 1.05493\n",
            " 92/136 [===================>..........] - ETA: 2:38 - loss: 1.1723 - accuracy: 0.6975\n",
            "Epoch 00002: loss improved from 1.05493 to 1.05451, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            " 93/136 [===================>..........] - ETA: 2:35 - loss: 1.1710 - accuracy: 0.6979\n",
            "Epoch 00002: loss improved from 1.05451 to 1.05231, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            " 94/136 [===================>..........] - ETA: 2:32 - loss: 1.1698 - accuracy: 0.6983\n",
            "Epoch 00002: loss did not improve from 1.05231\n",
            " 95/136 [===================>..........] - ETA: 2:28 - loss: 1.1686 - accuracy: 0.6987\n",
            "Epoch 00002: loss did not improve from 1.05231\n",
            " 96/136 [====================>.........] - ETA: 2:25 - loss: 1.1674 - accuracy: 0.6991\n",
            "Epoch 00002: loss did not improve from 1.05231\n",
            " 97/136 [====================>.........] - ETA: 2:21 - loss: 1.1662 - accuracy: 0.6994\n",
            "Epoch 00002: loss did not improve from 1.05231\n",
            " 98/136 [====================>.........] - ETA: 2:17 - loss: 1.1651 - accuracy: 0.6998\n",
            "Epoch 00002: loss did not improve from 1.05231\n",
            " 99/136 [====================>.........] - ETA: 2:14 - loss: 1.1639 - accuracy: 0.7001\n",
            "Epoch 00002: loss improved from 1.05231 to 1.04850, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "100/136 [=====================>........] - ETA: 2:10 - loss: 1.1627 - accuracy: 0.7005\n",
            "Epoch 00002: loss improved from 1.04850 to 1.04726, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "101/136 [=====================>........] - ETA: 2:07 - loss: 1.1616 - accuracy: 0.7008\n",
            "Epoch 00002: loss did not improve from 1.04726\n",
            "102/136 [=====================>........] - ETA: 2:03 - loss: 1.1605 - accuracy: 0.7011\n",
            "Epoch 00002: loss did not improve from 1.04726\n",
            "103/136 [=====================>........] - ETA: 2:00 - loss: 1.1594 - accuracy: 0.7014\n",
            "Epoch 00002: loss did not improve from 1.04726\n",
            "104/136 [=====================>........] - ETA: 1:56 - loss: 1.1584 - accuracy: 0.7017\n",
            "Epoch 00002: loss did not improve from 1.04726\n",
            "105/136 [======================>.......] - ETA: 1:52 - loss: 1.1573 - accuracy: 0.7020\n",
            "Epoch 00002: loss did not improve from 1.04726\n",
            "106/136 [======================>.......] - ETA: 1:48 - loss: 1.1563 - accuracy: 0.7023\n",
            "Epoch 00002: loss did not improve from 1.04726\n",
            "107/136 [======================>.......] - ETA: 1:45 - loss: 1.1553 - accuracy: 0.7025\n",
            "Epoch 00002: loss did not improve from 1.04726\n",
            "108/136 [======================>.......] - ETA: 1:41 - loss: 1.1543 - accuracy: 0.7028\n",
            "Epoch 00002: loss improved from 1.04726 to 1.04572, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "109/136 [=======================>......] - ETA: 1:38 - loss: 1.1533 - accuracy: 0.7031\n",
            "Epoch 00002: loss improved from 1.04572 to 1.04426, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "110/136 [=======================>......] - ETA: 1:34 - loss: 1.1523 - accuracy: 0.7033\n",
            "Epoch 00002: loss did not improve from 1.04426\n",
            "111/136 [=======================>......] - ETA: 1:31 - loss: 1.1514 - accuracy: 0.7035\n",
            "Epoch 00002: loss did not improve from 1.04426\n",
            "112/136 [=======================>......] - ETA: 1:28 - loss: 1.1504 - accuracy: 0.7038\n",
            "Epoch 00002: loss did not improve from 1.04426\n",
            "113/136 [=======================>......] - ETA: 1:24 - loss: 1.1495 - accuracy: 0.7040\n",
            "Epoch 00002: loss did not improve from 1.04426\n",
            "114/136 [========================>.....] - ETA: 1:20 - loss: 1.1486 - accuracy: 0.7042\n",
            "Epoch 00002: loss improved from 1.04426 to 1.04298, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "115/136 [========================>.....] - ETA: 1:17 - loss: 1.1476 - accuracy: 0.7045\n",
            "Epoch 00002: loss improved from 1.04298 to 1.04174, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "116/136 [========================>.....] - ETA: 1:13 - loss: 1.1467 - accuracy: 0.7047\n",
            "Epoch 00002: loss improved from 1.04174 to 1.04100, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "117/136 [========================>.....] - ETA: 1:10 - loss: 1.1458 - accuracy: 0.7049\n",
            "Epoch 00002: loss improved from 1.04100 to 1.03562, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "118/136 [=========================>....] - ETA: 1:06 - loss: 1.1448 - accuracy: 0.7052\n",
            "Epoch 00002: loss improved from 1.03562 to 1.03341, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "119/136 [=========================>....] - ETA: 1:02 - loss: 1.1438 - accuracy: 0.7054\n",
            "Epoch 00002: loss improved from 1.03341 to 1.02979, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "120/136 [=========================>....] - ETA: 59s - loss: 1.1429 - accuracy: 0.7056 \n",
            "Epoch 00002: loss did not improve from 1.02979\n",
            "121/136 [=========================>....] - ETA: 55s - loss: 1.1420 - accuracy: 0.7058\n",
            "Epoch 00002: loss did not improve from 1.02979\n",
            "122/136 [=========================>....] - ETA: 51s - loss: 1.1411 - accuracy: 0.7060\n",
            "Epoch 00002: loss did not improve from 1.02979\n",
            "123/136 [==========================>...] - ETA: 48s - loss: 1.1402 - accuracy: 0.7062\n",
            "Epoch 00002: loss improved from 1.02979 to 1.02801, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "124/136 [==========================>...] - ETA: 44s - loss: 1.1392 - accuracy: 0.7064\n",
            "Epoch 00002: loss improved from 1.02801 to 1.02560, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "125/136 [==========================>...] - ETA: 40s - loss: 1.1383 - accuracy: 0.7066\n",
            "Epoch 00002: loss improved from 1.02560 to 1.02501, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "126/136 [==========================>...] - ETA: 37s - loss: 1.1374 - accuracy: 0.7068\n",
            "Epoch 00002: loss improved from 1.02501 to 1.02073, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "127/136 [===========================>..] - ETA: 33s - loss: 1.1365 - accuracy: 0.7070\n",
            "Epoch 00002: loss improved from 1.02073 to 1.01852, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "128/136 [===========================>..] - ETA: 29s - loss: 1.1355 - accuracy: 0.7072\n",
            "Epoch 00002: loss did not improve from 1.01852\n",
            "129/136 [===========================>..] - ETA: 26s - loss: 1.1346 - accuracy: 0.7073\n",
            "Epoch 00002: loss improved from 1.01852 to 1.01784, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "130/136 [===========================>..] - ETA: 22s - loss: 1.1337 - accuracy: 0.7075\n",
            "Epoch 00002: loss improved from 1.01784 to 1.01672, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "131/136 [===========================>..] - ETA: 18s - loss: 1.1328 - accuracy: 0.7077\n",
            "Epoch 00002: loss did not improve from 1.01672\n",
            "132/136 [============================>.] - ETA: 14s - loss: 1.1320 - accuracy: 0.7078\n",
            "Epoch 00002: loss did not improve from 1.01672\n",
            "133/136 [============================>.] - ETA: 11s - loss: 1.1312 - accuracy: 0.7080\n",
            "Epoch 00002: loss did not improve from 1.01672\n",
            "134/136 [============================>.] - ETA: 7s - loss: 1.1303 - accuracy: 0.7082 \n",
            "Epoch 00002: loss did not improve from 1.01672\n",
            "135/136 [============================>.] - ETA: 3s - loss: 1.1295 - accuracy: 0.7083\n",
            "Epoch 00002: loss improved from 1.01672 to 1.01634, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "136/136 [==============================] - ETA: 0s - loss: 1.1286 - accuracy: 0.7085\n",
            "Epoch 00002: loss improved from 1.01634 to 1.01075, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "136/136 [==============================] - 553s 4s/step - loss: 1.1277 - accuracy: 0.7087 - val_loss: 1.2708 - val_accuracy: 0.7583\n",
            "Epoch 3/10\n",
            "  1/136 [..............................] - ETA: 12:23 - loss: 0.5539 - accuracy: 0.9000\n",
            "Epoch 00003: loss improved from 1.01075 to 0.55393, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "  2/136 [..............................] - ETA: 13:22 - loss: 0.7089 - accuracy: 0.8250\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "  3/136 [..............................] - ETA: 10:59 - loss: 0.7328 - accuracy: 0.8056\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "  4/136 [..............................] - ETA: 9:55 - loss: 0.7529 - accuracy: 0.7917 \n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "  5/136 [>.............................] - ETA: 9:14 - loss: 0.7801 - accuracy: 0.7773\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "  6/136 [>.............................] - ETA: 8:47 - loss: 0.7962 - accuracy: 0.7700\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "  7/136 [>.............................] - ETA: 8:29 - loss: 0.8063 - accuracy: 0.7661\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "  8/136 [>.............................] - ETA: 8:12 - loss: 0.8076 - accuracy: 0.7657\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "  9/136 [>.............................] - ETA: 8:01 - loss: 0.8069 - accuracy: 0.7658\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 10/136 [=>............................] - ETA: 7:53 - loss: 0.8060 - accuracy: 0.7662\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 11/136 [=>............................] - ETA: 7:43 - loss: 0.8058 - accuracy: 0.7668\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 12/136 [=>............................] - ETA: 7:36 - loss: 0.8024 - accuracy: 0.7689\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 13/136 [=>............................] - ETA: 7:29 - loss: 0.7986 - accuracy: 0.7701\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 14/136 [==>...........................] - ETA: 7:24 - loss: 0.7965 - accuracy: 0.7707\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 15/136 [==>...........................] - ETA: 7:18 - loss: 0.7945 - accuracy: 0.7713\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 16/136 [==>...........................] - ETA: 7:13 - loss: 0.7930 - accuracy: 0.7719\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 17/136 [==>...........................] - ETA: 7:06 - loss: 0.7924 - accuracy: 0.7718\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 18/136 [==>...........................] - ETA: 7:00 - loss: 0.7925 - accuracy: 0.7719\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 19/136 [===>..........................] - ETA: 6:57 - loss: 0.7926 - accuracy: 0.7720\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 20/136 [===>..........................] - ETA: 6:54 - loss: 0.7928 - accuracy: 0.7721\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 21/136 [===>..........................] - ETA: 6:48 - loss: 0.7934 - accuracy: 0.7721\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 22/136 [===>..........................] - ETA: 6:44 - loss: 0.7944 - accuracy: 0.7717\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 23/136 [====>.........................] - ETA: 6:40 - loss: 0.7955 - accuracy: 0.7714\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 24/136 [====>.........................] - ETA: 6:38 - loss: 0.7973 - accuracy: 0.7710\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 25/136 [====>.........................] - ETA: 6:34 - loss: 0.8099 - accuracy: 0.7703\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 26/136 [====>.........................] - ETA: 6:29 - loss: 0.8215 - accuracy: 0.7697\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 27/136 [====>.........................] - ETA: 6:24 - loss: 0.8323 - accuracy: 0.7690\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 28/136 [=====>........................] - ETA: 6:20 - loss: 0.8417 - accuracy: 0.7686\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 29/136 [=====>........................] - ETA: 6:16 - loss: 0.8505 - accuracy: 0.7680\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 30/136 [=====>........................] - ETA: 6:12 - loss: 0.8589 - accuracy: 0.7673\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 31/136 [=====>........................] - ETA: 6:08 - loss: 0.8671 - accuracy: 0.7664\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 32/136 [======>.......................] - ETA: 6:04 - loss: 0.8743 - accuracy: 0.7657\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 33/136 [======>.......................] - ETA: 6:00 - loss: 0.8813 - accuracy: 0.7649\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 34/136 [======>.......................] - ETA: 5:56 - loss: 0.8878 - accuracy: 0.7641\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 35/136 [======>.......................] - ETA: 5:52 - loss: 0.8937 - accuracy: 0.7635\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 36/136 [======>.......................] - ETA: 5:48 - loss: 0.8992 - accuracy: 0.7629\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 37/136 [=======>......................] - ETA: 5:44 - loss: 0.9048 - accuracy: 0.7621\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 38/136 [=======>......................] - ETA: 5:41 - loss: 0.9102 - accuracy: 0.7613\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 39/136 [=======>......................] - ETA: 5:37 - loss: 0.9150 - accuracy: 0.7607\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 40/136 [=======>......................] - ETA: 5:33 - loss: 0.9195 - accuracy: 0.7600\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 41/136 [========>.....................] - ETA: 5:29 - loss: 0.9237 - accuracy: 0.7594\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 42/136 [========>.....................] - ETA: 5:26 - loss: 0.9277 - accuracy: 0.7587\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 43/136 [========>.....................] - ETA: 5:22 - loss: 0.9316 - accuracy: 0.7581\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 44/136 [========>.....................] - ETA: 5:19 - loss: 0.9352 - accuracy: 0.7574\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 45/136 [========>.....................] - ETA: 5:15 - loss: 0.9385 - accuracy: 0.7569\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 46/136 [=========>....................] - ETA: 5:11 - loss: 0.9417 - accuracy: 0.7563\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 47/136 [=========>....................] - ETA: 5:08 - loss: 0.9446 - accuracy: 0.7558\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 48/136 [=========>....................] - ETA: 5:04 - loss: 0.9474 - accuracy: 0.7552\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 49/136 [=========>....................] - ETA: 5:00 - loss: 0.9501 - accuracy: 0.7546\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 50/136 [==========>...................] - ETA: 4:57 - loss: 0.9524 - accuracy: 0.7541\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 51/136 [==========>...................] - ETA: 4:53 - loss: 0.9612 - accuracy: 0.7535\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 52/136 [==========>...................] - ETA: 4:49 - loss: 0.9693 - accuracy: 0.7531\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 53/136 [==========>...................] - ETA: 4:45 - loss: 0.9769 - accuracy: 0.7526\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 54/136 [==========>...................] - ETA: 4:42 - loss: 0.9839 - accuracy: 0.7523\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 55/136 [===========>..................] - ETA: 4:39 - loss: 0.9904 - accuracy: 0.7520\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 56/136 [===========>..................] - ETA: 4:35 - loss: 0.9964 - accuracy: 0.7518\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 57/136 [===========>..................] - ETA: 4:32 - loss: 1.0021 - accuracy: 0.7516\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 58/136 [===========>..................] - ETA: 4:29 - loss: 1.0075 - accuracy: 0.7514\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 59/136 [============>.................] - ETA: 4:25 - loss: 1.0125 - accuracy: 0.7512\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 60/136 [============>.................] - ETA: 4:21 - loss: 1.0175 - accuracy: 0.7509\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 61/136 [============>.................] - ETA: 4:18 - loss: 1.0221 - accuracy: 0.7507\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 62/136 [============>.................] - ETA: 4:15 - loss: 1.0264 - accuracy: 0.7505\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 63/136 [============>.................] - ETA: 4:11 - loss: 1.0305 - accuracy: 0.7503\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 64/136 [=============>................] - ETA: 4:08 - loss: 1.0344 - accuracy: 0.7501\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 65/136 [=============>................] - ETA: 4:04 - loss: 1.0380 - accuracy: 0.7499\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 66/136 [=============>................] - ETA: 4:01 - loss: 1.0414 - accuracy: 0.7498\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 67/136 [=============>................] - ETA: 3:57 - loss: 1.0445 - accuracy: 0.7497\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 68/136 [==============>...............] - ETA: 3:54 - loss: 1.0475 - accuracy: 0.7495\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 69/136 [==============>...............] - ETA: 3:50 - loss: 1.0502 - accuracy: 0.7495\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 70/136 [==============>...............] - ETA: 3:47 - loss: 1.0528 - accuracy: 0.7493\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 71/136 [==============>...............] - ETA: 3:43 - loss: 1.0554 - accuracy: 0.7492\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 72/136 [==============>...............] - ETA: 3:40 - loss: 1.0579 - accuracy: 0.7491\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 73/136 [===============>..............] - ETA: 3:36 - loss: 1.0602 - accuracy: 0.7490\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 74/136 [===============>..............] - ETA: 3:33 - loss: 1.0623 - accuracy: 0.7489\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 75/136 [===============>..............] - ETA: 3:29 - loss: 1.0643 - accuracy: 0.7489\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 76/136 [===============>..............] - ETA: 3:26 - loss: 1.0663 - accuracy: 0.7488\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 77/136 [===============>..............] - ETA: 3:23 - loss: 1.0681 - accuracy: 0.7487\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 78/136 [================>.............] - ETA: 3:19 - loss: 1.0697 - accuracy: 0.7486\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 79/136 [================>.............] - ETA: 3:16 - loss: 1.0713 - accuracy: 0.7485\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 80/136 [================>.............] - ETA: 3:12 - loss: 1.0727 - accuracy: 0.7485\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 81/136 [================>.............] - ETA: 3:09 - loss: 1.0741 - accuracy: 0.7484\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 82/136 [=================>............] - ETA: 3:05 - loss: 1.0754 - accuracy: 0.7484\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 83/136 [=================>............] - ETA: 3:02 - loss: 1.0766 - accuracy: 0.7483\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 84/136 [=================>............] - ETA: 2:58 - loss: 1.0777 - accuracy: 0.7483\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 85/136 [=================>............] - ETA: 2:55 - loss: 1.0788 - accuracy: 0.7482\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 86/136 [=================>............] - ETA: 2:51 - loss: 1.0799 - accuracy: 0.7481\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 87/136 [==================>...........] - ETA: 2:48 - loss: 1.0810 - accuracy: 0.7481\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 88/136 [==================>...........] - ETA: 2:44 - loss: 1.0820 - accuracy: 0.7480\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 89/136 [==================>...........] - ETA: 2:41 - loss: 1.0828 - accuracy: 0.7479\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 90/136 [==================>...........] - ETA: 2:37 - loss: 1.0836 - accuracy: 0.7479\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 91/136 [===================>..........] - ETA: 2:34 - loss: 1.0844 - accuracy: 0.7479\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 92/136 [===================>..........] - ETA: 2:31 - loss: 1.0850 - accuracy: 0.7479\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 93/136 [===================>..........] - ETA: 2:27 - loss: 1.0856 - accuracy: 0.7479\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 94/136 [===================>..........] - ETA: 2:24 - loss: 1.0861 - accuracy: 0.7479\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 95/136 [===================>..........] - ETA: 2:20 - loss: 1.0866 - accuracy: 0.7479\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 96/136 [====================>.........] - ETA: 2:17 - loss: 1.0869 - accuracy: 0.7479\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 97/136 [====================>.........] - ETA: 2:14 - loss: 1.0872 - accuracy: 0.7479\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 98/136 [====================>.........] - ETA: 2:10 - loss: 1.0875 - accuracy: 0.7480\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            " 99/136 [====================>.........] - ETA: 2:07 - loss: 1.0877 - accuracy: 0.7480\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "100/136 [=====================>........] - ETA: 2:04 - loss: 1.0879 - accuracy: 0.7480\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "101/136 [=====================>........] - ETA: 2:01 - loss: 1.0880 - accuracy: 0.7481\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "102/136 [=====================>........] - ETA: 1:57 - loss: 1.0881 - accuracy: 0.7482\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "103/136 [=====================>........] - ETA: 1:54 - loss: 1.0881 - accuracy: 0.7483\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "104/136 [=====================>........] - ETA: 1:51 - loss: 1.0881 - accuracy: 0.7484\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "105/136 [======================>.......] - ETA: 1:47 - loss: 1.0881 - accuracy: 0.7485\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "106/136 [======================>.......] - ETA: 1:44 - loss: 1.0881 - accuracy: 0.7486\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "107/136 [======================>.......] - ETA: 1:40 - loss: 1.0881 - accuracy: 0.7486\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "108/136 [======================>.......] - ETA: 1:37 - loss: 1.0882 - accuracy: 0.7487\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "109/136 [=======================>......] - ETA: 1:33 - loss: 1.0883 - accuracy: 0.7487\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "110/136 [=======================>......] - ETA: 1:30 - loss: 1.0882 - accuracy: 0.7488\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "111/136 [=======================>......] - ETA: 1:26 - loss: 1.0882 - accuracy: 0.7488\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "112/136 [=======================>......] - ETA: 1:23 - loss: 1.0881 - accuracy: 0.7488\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "113/136 [=======================>......] - ETA: 1:19 - loss: 1.0881 - accuracy: 0.7489\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "114/136 [========================>.....] - ETA: 1:16 - loss: 1.0880 - accuracy: 0.7489\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "115/136 [========================>.....] - ETA: 1:12 - loss: 1.0879 - accuracy: 0.7489\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "116/136 [========================>.....] - ETA: 1:08 - loss: 1.0877 - accuracy: 0.7490\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "117/136 [========================>.....] - ETA: 1:05 - loss: 1.0875 - accuracy: 0.7490\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "118/136 [=========================>....] - ETA: 1:02 - loss: 1.0874 - accuracy: 0.7491\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "119/136 [=========================>....] - ETA: 58s - loss: 1.0872 - accuracy: 0.7491 \n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "120/136 [=========================>....] - ETA: 55s - loss: 1.0871 - accuracy: 0.7491\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "121/136 [=========================>....] - ETA: 51s - loss: 1.0869 - accuracy: 0.7491\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "122/136 [=========================>....] - ETA: 48s - loss: 1.0867 - accuracy: 0.7492\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "123/136 [==========================>...] - ETA: 44s - loss: 1.0865 - accuracy: 0.7492\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "124/136 [==========================>...] - ETA: 41s - loss: 1.0862 - accuracy: 0.7492\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "125/136 [==========================>...] - ETA: 37s - loss: 1.0860 - accuracy: 0.7493\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "126/136 [==========================>...] - ETA: 34s - loss: 1.0858 - accuracy: 0.7493\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "127/136 [===========================>..] - ETA: 31s - loss: 1.0855 - accuracy: 0.7494\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "128/136 [===========================>..] - ETA: 27s - loss: 1.0853 - accuracy: 0.7494\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "129/136 [===========================>..] - ETA: 24s - loss: 1.0852 - accuracy: 0.7494\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "130/136 [===========================>..] - ETA: 20s - loss: 1.0850 - accuracy: 0.7494\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "131/136 [===========================>..] - ETA: 17s - loss: 1.0848 - accuracy: 0.7494\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "132/136 [============================>.] - ETA: 13s - loss: 1.0846 - accuracy: 0.7494\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "133/136 [============================>.] - ETA: 10s - loss: 1.0844 - accuracy: 0.7494\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "134/136 [============================>.] - ETA: 6s - loss: 1.0841 - accuracy: 0.7494 \n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "135/136 [============================>.] - ETA: 3s - loss: 1.0839 - accuracy: 0.7494\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "136/136 [==============================] - ETA: 0s - loss: 1.0837 - accuracy: 0.7494\n",
            "Epoch 00003: loss did not improve from 0.55393\n",
            "136/136 [==============================] - 519s 4s/step - loss: 1.0835 - accuracy: 0.7494 - val_loss: 0.9551 - val_accuracy: 0.7833\n",
            "Epoch 4/10\n",
            "  1/136 [..............................] - ETA: 12:00 - loss: 0.7495 - accuracy: 0.8000\n",
            "Epoch 00004: loss did not improve from 0.55393\n",
            "  2/136 [..............................] - ETA: 8:28 - loss: 0.7161 - accuracy: 0.8000 \n",
            "Epoch 00004: loss did not improve from 0.55393\n",
            "  3/136 [..............................] - ETA: 8:44 - loss: 0.7461 - accuracy: 0.7889\n",
            "Epoch 00004: loss did not improve from 0.55393\n",
            "  4/136 [..............................] - ETA: 8:28 - loss: 0.7421 - accuracy: 0.7917\n",
            "Epoch 00004: loss did not improve from 0.55393\n",
            "  5/136 [>.............................] - ETA: 8:25 - loss: 0.7267 - accuracy: 0.7973\n",
            "Epoch 00004: loss did not improve from 0.55393\n",
            "  6/136 [>.............................] - ETA: 8:13 - loss: 0.7073 - accuracy: 0.8061\n",
            "Epoch 00004: loss did not improve from 0.55393\n",
            "  7/136 [>.............................] - ETA: 8:02 - loss: 0.6851 - accuracy: 0.8154\n",
            "Epoch 00004: loss improved from 0.55393 to 0.55175, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "  8/136 [>.............................] - ETA: 8:39 - loss: 0.6646 - accuracy: 0.8244\n",
            "Epoch 00004: loss improved from 0.55175 to 0.52112, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "  9/136 [>.............................] - ETA: 8:38 - loss: 0.6496 - accuracy: 0.8316\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 10/136 [=>............................] - ETA: 8:23 - loss: 0.6378 - accuracy: 0.8374\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 11/136 [=>............................] - ETA: 8:10 - loss: 0.6299 - accuracy: 0.8415\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 12/136 [=>............................] - ETA: 8:05 - loss: 0.6308 - accuracy: 0.8429\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 13/136 [=>............................] - ETA: 8:00 - loss: 0.6282 - accuracy: 0.8449\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 14/136 [==>...........................] - ETA: 7:51 - loss: 0.6285 - accuracy: 0.8453\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 15/136 [==>...........................] - ETA: 7:41 - loss: 0.6316 - accuracy: 0.8449\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 16/136 [==>...........................] - ETA: 7:33 - loss: 0.6346 - accuracy: 0.8449\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 17/136 [==>...........................] - ETA: 7:26 - loss: 0.6381 - accuracy: 0.8439\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 18/136 [==>...........................] - ETA: 7:20 - loss: 0.6399 - accuracy: 0.8437\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 19/136 [===>..........................] - ETA: 7:14 - loss: 0.6416 - accuracy: 0.8433\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 20/136 [===>..........................] - ETA: 7:09 - loss: 0.6426 - accuracy: 0.8431\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 21/136 [===>..........................] - ETA: 7:03 - loss: 0.6436 - accuracy: 0.8429\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 22/136 [===>..........................] - ETA: 6:58 - loss: 0.6448 - accuracy: 0.8424\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 23/136 [====>.........................] - ETA: 6:53 - loss: 0.6459 - accuracy: 0.8419\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 24/136 [====>.........................] - ETA: 6:48 - loss: 0.6479 - accuracy: 0.8410\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 25/136 [====>.........................] - ETA: 6:43 - loss: 0.6501 - accuracy: 0.8400\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 26/136 [====>.........................] - ETA: 6:39 - loss: 0.6525 - accuracy: 0.8391\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 27/136 [====>.........................] - ETA: 6:34 - loss: 0.6551 - accuracy: 0.8379\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 28/136 [=====>........................] - ETA: 6:30 - loss: 0.6580 - accuracy: 0.8367\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 29/136 [=====>........................] - ETA: 6:25 - loss: 0.6607 - accuracy: 0.8355\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 30/136 [=====>........................] - ETA: 6:20 - loss: 0.6632 - accuracy: 0.8344\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 31/136 [=====>........................] - ETA: 6:16 - loss: 0.6660 - accuracy: 0.8332\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 32/136 [======>.......................] - ETA: 6:11 - loss: 0.6688 - accuracy: 0.8320\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 33/136 [======>.......................] - ETA: 6:06 - loss: 0.6714 - accuracy: 0.8308\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 34/136 [======>.......................] - ETA: 6:01 - loss: 0.6745 - accuracy: 0.8295\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 35/136 [======>.......................] - ETA: 5:57 - loss: 0.6776 - accuracy: 0.8281\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 36/136 [======>.......................] - ETA: 5:53 - loss: 0.6806 - accuracy: 0.8267\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 37/136 [=======>......................] - ETA: 5:49 - loss: 0.6834 - accuracy: 0.8254\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 38/136 [=======>......................] - ETA: 5:45 - loss: 0.6869 - accuracy: 0.8237\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 39/136 [=======>......................] - ETA: 5:42 - loss: 0.6902 - accuracy: 0.8221\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 40/136 [=======>......................] - ETA: 5:38 - loss: 0.6936 - accuracy: 0.8205\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 41/136 [========>.....................] - ETA: 5:34 - loss: 0.6972 - accuracy: 0.8188\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 42/136 [========>.....................] - ETA: 5:30 - loss: 0.7008 - accuracy: 0.8171\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 43/136 [========>.....................] - ETA: 5:26 - loss: 0.7040 - accuracy: 0.8156\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 44/136 [========>.....................] - ETA: 5:23 - loss: 0.7070 - accuracy: 0.8142\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 45/136 [========>.....................] - ETA: 5:19 - loss: 0.7097 - accuracy: 0.8129\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 46/136 [=========>....................] - ETA: 5:15 - loss: 0.7125 - accuracy: 0.8116\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 47/136 [=========>....................] - ETA: 5:12 - loss: 0.7150 - accuracy: 0.8104\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 48/136 [=========>....................] - ETA: 5:08 - loss: 0.7174 - accuracy: 0.8091\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 49/136 [=========>....................] - ETA: 5:04 - loss: 0.7194 - accuracy: 0.8081\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 50/136 [==========>...................] - ETA: 5:00 - loss: 0.7213 - accuracy: 0.8071\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 51/136 [==========>...................] - ETA: 4:56 - loss: 0.7230 - accuracy: 0.8062\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 52/136 [==========>...................] - ETA: 4:53 - loss: 0.7244 - accuracy: 0.8054\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 53/136 [==========>...................] - ETA: 4:49 - loss: 0.7257 - accuracy: 0.8048\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 54/136 [==========>...................] - ETA: 4:45 - loss: 0.7270 - accuracy: 0.8040\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 55/136 [===========>..................] - ETA: 4:41 - loss: 0.7284 - accuracy: 0.8032\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 56/136 [===========>..................] - ETA: 4:37 - loss: 0.7298 - accuracy: 0.8024\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 57/136 [===========>..................] - ETA: 4:34 - loss: 0.7311 - accuracy: 0.8017\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 58/136 [===========>..................] - ETA: 4:30 - loss: 0.7323 - accuracy: 0.8010\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 59/136 [============>.................] - ETA: 4:27 - loss: 0.7337 - accuracy: 0.8003\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 60/136 [============>.................] - ETA: 4:23 - loss: 0.7351 - accuracy: 0.7996\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 61/136 [============>.................] - ETA: 4:20 - loss: 0.7366 - accuracy: 0.7988\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 62/136 [============>.................] - ETA: 4:16 - loss: 0.7381 - accuracy: 0.7981\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 63/136 [============>.................] - ETA: 4:12 - loss: 0.7396 - accuracy: 0.7974\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 64/136 [=============>................] - ETA: 4:09 - loss: 0.7409 - accuracy: 0.7967\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 65/136 [=============>................] - ETA: 4:05 - loss: 0.7422 - accuracy: 0.7960\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 66/136 [=============>................] - ETA: 4:02 - loss: 0.7434 - accuracy: 0.7954\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 67/136 [=============>................] - ETA: 3:58 - loss: 0.7447 - accuracy: 0.7948\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 68/136 [==============>...............] - ETA: 3:55 - loss: 0.7457 - accuracy: 0.7942\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 69/136 [==============>...............] - ETA: 3:51 - loss: 0.7467 - accuracy: 0.7937\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 70/136 [==============>...............] - ETA: 3:48 - loss: 0.7476 - accuracy: 0.7932\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 71/136 [==============>...............] - ETA: 3:44 - loss: 0.7483 - accuracy: 0.7927\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 72/136 [==============>...............] - ETA: 3:41 - loss: 0.7490 - accuracy: 0.7923\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 73/136 [===============>..............] - ETA: 3:37 - loss: 0.7496 - accuracy: 0.7918\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 74/136 [===============>..............] - ETA: 3:34 - loss: 0.7502 - accuracy: 0.7914\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 75/136 [===============>..............] - ETA: 3:29 - loss: 0.7507 - accuracy: 0.7911\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 76/136 [===============>..............] - ETA: 3:26 - loss: 0.7512 - accuracy: 0.7907\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 77/136 [===============>..............] - ETA: 3:22 - loss: 0.7518 - accuracy: 0.7903\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 78/136 [================>.............] - ETA: 3:17 - loss: 0.7523 - accuracy: 0.7899\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 79/136 [================>.............] - ETA: 3:14 - loss: 0.7528 - accuracy: 0.7895\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 80/136 [================>.............] - ETA: 3:11 - loss: 0.7532 - accuracy: 0.7892\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 81/136 [================>.............] - ETA: 3:07 - loss: 0.7536 - accuracy: 0.7889\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 82/136 [=================>............] - ETA: 3:04 - loss: 0.7539 - accuracy: 0.7886\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 83/136 [=================>............] - ETA: 3:01 - loss: 0.7542 - accuracy: 0.7884\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 84/136 [=================>............] - ETA: 2:57 - loss: 0.7544 - accuracy: 0.7882\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 85/136 [=================>............] - ETA: 2:54 - loss: 0.7546 - accuracy: 0.7880\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 86/136 [=================>............] - ETA: 2:50 - loss: 0.7548 - accuracy: 0.7878\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 87/136 [==================>...........] - ETA: 2:47 - loss: 0.7550 - accuracy: 0.7876\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 88/136 [==================>...........] - ETA: 2:44 - loss: 0.7553 - accuracy: 0.7874\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 89/136 [==================>...........] - ETA: 2:40 - loss: 0.7555 - accuracy: 0.7872\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 90/136 [==================>...........] - ETA: 2:37 - loss: 0.7557 - accuracy: 0.7869\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 91/136 [===================>..........] - ETA: 2:33 - loss: 0.7560 - accuracy: 0.7867\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 92/136 [===================>..........] - ETA: 2:30 - loss: 0.7562 - accuracy: 0.7865\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 93/136 [===================>..........] - ETA: 2:26 - loss: 0.7563 - accuracy: 0.7864\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 94/136 [===================>..........] - ETA: 2:23 - loss: 0.7565 - accuracy: 0.7862\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 95/136 [===================>..........] - ETA: 2:20 - loss: 0.7566 - accuracy: 0.7860\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 96/136 [====================>.........] - ETA: 2:16 - loss: 0.7567 - accuracy: 0.7859\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 97/136 [====================>.........] - ETA: 2:13 - loss: 0.7568 - accuracy: 0.7857\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 98/136 [====================>.........] - ETA: 2:09 - loss: 0.7569 - accuracy: 0.7856\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            " 99/136 [====================>.........] - ETA: 2:06 - loss: 0.7569 - accuracy: 0.7855\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "100/136 [=====================>........] - ETA: 2:02 - loss: 0.7569 - accuracy: 0.7855\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "101/136 [=====================>........] - ETA: 1:59 - loss: 0.7569 - accuracy: 0.7854\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "102/136 [=====================>........] - ETA: 1:56 - loss: 0.7569 - accuracy: 0.7853\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "103/136 [=====================>........] - ETA: 1:52 - loss: 0.7569 - accuracy: 0.7852\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "104/136 [=====================>........] - ETA: 1:49 - loss: 0.7569 - accuracy: 0.7852\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "105/136 [======================>.......] - ETA: 1:45 - loss: 0.7568 - accuracy: 0.7851\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "106/136 [======================>.......] - ETA: 1:42 - loss: 0.7568 - accuracy: 0.7851\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "107/136 [======================>.......] - ETA: 1:38 - loss: 0.7567 - accuracy: 0.7851\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "108/136 [======================>.......] - ETA: 1:35 - loss: 0.7566 - accuracy: 0.7850\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "109/136 [=======================>......] - ETA: 1:32 - loss: 0.7565 - accuracy: 0.7850\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "110/136 [=======================>......] - ETA: 1:28 - loss: 0.7564 - accuracy: 0.7849\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "111/136 [=======================>......] - ETA: 1:25 - loss: 0.7563 - accuracy: 0.7849\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "112/136 [=======================>......] - ETA: 1:21 - loss: 0.7563 - accuracy: 0.7849\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "113/136 [=======================>......] - ETA: 1:18 - loss: 0.7561 - accuracy: 0.7849\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "114/136 [========================>.....] - ETA: 1:14 - loss: 0.7560 - accuracy: 0.7849\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "115/136 [========================>.....] - ETA: 1:11 - loss: 0.7558 - accuracy: 0.7849\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "116/136 [========================>.....] - ETA: 1:08 - loss: 0.7557 - accuracy: 0.7848\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "117/136 [========================>.....] - ETA: 1:04 - loss: 0.7555 - accuracy: 0.7848\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "118/136 [=========================>....] - ETA: 1:01 - loss: 0.7553 - accuracy: 0.7849\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "119/136 [=========================>....] - ETA: 57s - loss: 0.7551 - accuracy: 0.7849 \n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "120/136 [=========================>....] - ETA: 54s - loss: 0.7549 - accuracy: 0.7849\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "121/136 [=========================>....] - ETA: 51s - loss: 0.7547 - accuracy: 0.7849\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "122/136 [=========================>....] - ETA: 47s - loss: 0.7544 - accuracy: 0.7850\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "123/136 [==========================>...] - ETA: 44s - loss: 0.7542 - accuracy: 0.7850\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "124/136 [==========================>...] - ETA: 40s - loss: 0.7540 - accuracy: 0.7850\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "125/136 [==========================>...] - ETA: 37s - loss: 0.7538 - accuracy: 0.7850\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "126/136 [==========================>...] - ETA: 34s - loss: 0.7536 - accuracy: 0.7850\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "127/136 [===========================>..] - ETA: 30s - loss: 0.7533 - accuracy: 0.7850\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "128/136 [===========================>..] - ETA: 27s - loss: 0.7531 - accuracy: 0.7851\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "129/136 [===========================>..] - ETA: 23s - loss: 0.7529 - accuracy: 0.7851\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "130/136 [===========================>..] - ETA: 20s - loss: 0.7527 - accuracy: 0.7851\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "131/136 [===========================>..] - ETA: 17s - loss: 0.7524 - accuracy: 0.7851\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "132/136 [============================>.] - ETA: 13s - loss: 0.7521 - accuracy: 0.7852\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "133/136 [============================>.] - ETA: 10s - loss: 0.7519 - accuracy: 0.7852\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "134/136 [============================>.] - ETA: 6s - loss: 0.7516 - accuracy: 0.7853 \n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "135/136 [============================>.] - ETA: 3s - loss: 0.7512 - accuracy: 0.7853\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "136/136 [==============================] - ETA: 0s - loss: 0.7509 - accuracy: 0.7854\n",
            "Epoch 00004: loss did not improve from 0.52112\n",
            "136/136 [==============================] - 545s 4s/step - loss: 0.7506 - accuracy: 0.7854 - val_loss: 0.7343 - val_accuracy: 0.8292\n",
            "Epoch 5/10\n",
            "  1/136 [..............................] - ETA: 13:32 - loss: 0.7019 - accuracy: 0.7000\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "  2/136 [..............................] - ETA: 8:38 - loss: 0.7940 - accuracy: 0.6500 \n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "  3/136 [..............................] - ETA: 8:28 - loss: 0.7715 - accuracy: 0.6667\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "  4/136 [..............................] - ETA: 8:04 - loss: 0.7529 - accuracy: 0.6813\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "  5/136 [>.............................] - ETA: 7:53 - loss: 0.7298 - accuracy: 0.6970\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "  6/136 [>.............................] - ETA: 7:43 - loss: 0.7096 - accuracy: 0.7086\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "  7/136 [>.............................] - ETA: 7:37 - loss: 0.6915 - accuracy: 0.7196\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "  8/136 [>.............................] - ETA: 7:34 - loss: 0.6947 - accuracy: 0.7265\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "  9/136 [>.............................] - ETA: 7:14 - loss: 0.6974 - accuracy: 0.7322\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 10/136 [=>............................] - ETA: 7:08 - loss: 0.6964 - accuracy: 0.7380\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 11/136 [=>............................] - ETA: 7:02 - loss: 0.6906 - accuracy: 0.7445\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 12/136 [=>............................] - ETA: 6:30 - loss: 0.6856 - accuracy: 0.7501\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 13/136 [=>............................] - ETA: 6:34 - loss: 0.6864 - accuracy: 0.7523\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 14/136 [==>...........................] - ETA: 6:32 - loss: 0.6880 - accuracy: 0.7538\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 15/136 [==>...........................] - ETA: 6:32 - loss: 0.6895 - accuracy: 0.7552\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 16/136 [==>...........................] - ETA: 6:31 - loss: 0.6917 - accuracy: 0.7561\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 17/136 [==>...........................] - ETA: 6:28 - loss: 0.6953 - accuracy: 0.7563\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 18/136 [==>...........................] - ETA: 6:25 - loss: 0.6992 - accuracy: 0.7559\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 19/136 [===>..........................] - ETA: 6:24 - loss: 0.7040 - accuracy: 0.7549\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 20/136 [===>..........................] - ETA: 6:22 - loss: 0.7089 - accuracy: 0.7539\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 21/136 [===>..........................] - ETA: 6:19 - loss: 0.7141 - accuracy: 0.7529\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 22/136 [===>..........................] - ETA: 6:16 - loss: 0.7190 - accuracy: 0.7519\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 23/136 [====>.........................] - ETA: 6:14 - loss: 0.7250 - accuracy: 0.7503\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 24/136 [====>.........................] - ETA: 6:11 - loss: 0.7309 - accuracy: 0.7487\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 25/136 [====>.........................] - ETA: 6:09 - loss: 0.7368 - accuracy: 0.7470\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 26/136 [====>.........................] - ETA: 6:05 - loss: 0.7424 - accuracy: 0.7456\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 27/136 [====>.........................] - ETA: 6:02 - loss: 0.7479 - accuracy: 0.7441\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 28/136 [=====>........................] - ETA: 6:00 - loss: 0.7525 - accuracy: 0.7430\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 29/136 [=====>........................] - ETA: 5:57 - loss: 0.7569 - accuracy: 0.7421\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 30/136 [=====>........................] - ETA: 5:54 - loss: 0.7608 - accuracy: 0.7415\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 31/136 [=====>........................] - ETA: 5:51 - loss: 0.7646 - accuracy: 0.7407\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 32/136 [======>.......................] - ETA: 5:48 - loss: 0.7686 - accuracy: 0.7399\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 33/136 [======>.......................] - ETA: 5:44 - loss: 0.7726 - accuracy: 0.7390\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 34/136 [======>.......................] - ETA: 5:41 - loss: 0.7763 - accuracy: 0.7382\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 35/136 [======>.......................] - ETA: 5:38 - loss: 0.7797 - accuracy: 0.7375\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 36/136 [======>.......................] - ETA: 5:35 - loss: 0.7831 - accuracy: 0.7367\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 37/136 [=======>......................] - ETA: 5:32 - loss: 0.7861 - accuracy: 0.7361\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 38/136 [=======>......................] - ETA: 5:29 - loss: 0.7889 - accuracy: 0.7355\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 39/136 [=======>......................] - ETA: 5:27 - loss: 0.7917 - accuracy: 0.7347\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 40/136 [=======>......................] - ETA: 5:24 - loss: 0.7946 - accuracy: 0.7340\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 41/136 [========>.....................] - ETA: 5:21 - loss: 0.7974 - accuracy: 0.7332\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 42/136 [========>.....................] - ETA: 5:18 - loss: 0.8001 - accuracy: 0.7326\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 43/136 [========>.....................] - ETA: 5:16 - loss: 0.8026 - accuracy: 0.7320\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 44/136 [========>.....................] - ETA: 5:13 - loss: 0.8051 - accuracy: 0.7314\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 45/136 [========>.....................] - ETA: 5:10 - loss: 0.8076 - accuracy: 0.7307\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 46/136 [=========>....................] - ETA: 5:07 - loss: 0.8097 - accuracy: 0.7302\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 47/136 [=========>....................] - ETA: 5:04 - loss: 0.8116 - accuracy: 0.7298\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 48/136 [=========>....................] - ETA: 5:01 - loss: 0.8133 - accuracy: 0.7294\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 49/136 [=========>....................] - ETA: 4:58 - loss: 0.8150 - accuracy: 0.7290\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 50/136 [==========>...................] - ETA: 4:54 - loss: 0.8165 - accuracy: 0.7287\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 51/136 [==========>...................] - ETA: 4:51 - loss: 0.8181 - accuracy: 0.7283\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 52/136 [==========>...................] - ETA: 4:47 - loss: 0.8197 - accuracy: 0.7280\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 53/136 [==========>...................] - ETA: 4:44 - loss: 0.8214 - accuracy: 0.7276\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 54/136 [==========>...................] - ETA: 4:41 - loss: 0.8230 - accuracy: 0.7271\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 55/136 [===========>..................] - ETA: 4:37 - loss: 0.8248 - accuracy: 0.7267\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 56/136 [===========>..................] - ETA: 4:34 - loss: 0.8264 - accuracy: 0.7262\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 57/136 [===========>..................] - ETA: 4:30 - loss: 0.8280 - accuracy: 0.7258\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 58/136 [===========>..................] - ETA: 4:26 - loss: 0.8296 - accuracy: 0.7254\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 59/136 [============>.................] - ETA: 4:23 - loss: 0.8311 - accuracy: 0.7250\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 60/136 [============>.................] - ETA: 4:19 - loss: 0.8324 - accuracy: 0.7246\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 61/136 [============>.................] - ETA: 4:16 - loss: 0.8337 - accuracy: 0.7244\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 62/136 [============>.................] - ETA: 4:12 - loss: 0.8348 - accuracy: 0.7241\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 63/136 [============>.................] - ETA: 4:09 - loss: 0.8360 - accuracy: 0.7239\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 64/136 [=============>................] - ETA: 4:06 - loss: 0.8370 - accuracy: 0.7237\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 65/136 [=============>................] - ETA: 4:02 - loss: 0.8380 - accuracy: 0.7235\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 66/136 [=============>................] - ETA: 3:59 - loss: 0.8389 - accuracy: 0.7233\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 67/136 [=============>................] - ETA: 3:55 - loss: 0.8397 - accuracy: 0.7232\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 68/136 [==============>...............] - ETA: 3:52 - loss: 0.8405 - accuracy: 0.7231\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 69/136 [==============>...............] - ETA: 3:49 - loss: 0.8412 - accuracy: 0.7230\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 70/136 [==============>...............] - ETA: 3:45 - loss: 0.8419 - accuracy: 0.7229\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 71/136 [==============>...............] - ETA: 3:42 - loss: 0.8426 - accuracy: 0.7228\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 72/136 [==============>...............] - ETA: 3:38 - loss: 0.8432 - accuracy: 0.7227\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 73/136 [===============>..............] - ETA: 3:34 - loss: 0.8436 - accuracy: 0.7227\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 74/136 [===============>..............] - ETA: 3:31 - loss: 0.8442 - accuracy: 0.7226\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 75/136 [===============>..............] - ETA: 3:28 - loss: 0.8447 - accuracy: 0.7226\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 76/136 [===============>..............] - ETA: 3:24 - loss: 0.8453 - accuracy: 0.7225\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 77/136 [===============>..............] - ETA: 3:21 - loss: 0.8460 - accuracy: 0.7223\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 78/136 [================>.............] - ETA: 3:17 - loss: 0.8467 - accuracy: 0.7222\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 79/136 [================>.............] - ETA: 3:14 - loss: 0.8475 - accuracy: 0.7220\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 80/136 [================>.............] - ETA: 3:10 - loss: 0.8482 - accuracy: 0.7218\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 81/136 [================>.............] - ETA: 3:07 - loss: 0.8488 - accuracy: 0.7217\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 82/136 [=================>............] - ETA: 3:03 - loss: 0.8494 - accuracy: 0.7215\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 83/136 [=================>............] - ETA: 3:00 - loss: 0.8499 - accuracy: 0.7214\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 84/136 [=================>............] - ETA: 2:57 - loss: 0.8504 - accuracy: 0.7213\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 85/136 [=================>............] - ETA: 2:53 - loss: 0.8509 - accuracy: 0.7212\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 86/136 [=================>............] - ETA: 2:50 - loss: 0.8514 - accuracy: 0.7211\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 87/136 [==================>...........] - ETA: 2:46 - loss: 0.8518 - accuracy: 0.7210\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 88/136 [==================>...........] - ETA: 2:43 - loss: 0.8522 - accuracy: 0.7210\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 89/136 [==================>...........] - ETA: 2:39 - loss: 0.8525 - accuracy: 0.7209\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 90/136 [==================>...........] - ETA: 2:36 - loss: 0.8528 - accuracy: 0.7209\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 91/136 [===================>..........] - ETA: 2:33 - loss: 0.8531 - accuracy: 0.7209\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 92/136 [===================>..........] - ETA: 2:29 - loss: 0.8534 - accuracy: 0.7209\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 93/136 [===================>..........] - ETA: 2:26 - loss: 0.8536 - accuracy: 0.7209\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 94/136 [===================>..........] - ETA: 2:22 - loss: 0.8538 - accuracy: 0.7210\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 95/136 [===================>..........] - ETA: 2:19 - loss: 0.8539 - accuracy: 0.7211\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 96/136 [====================>.........] - ETA: 2:16 - loss: 0.8540 - accuracy: 0.7211\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 97/136 [====================>.........] - ETA: 2:12 - loss: 0.8541 - accuracy: 0.7212\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 98/136 [====================>.........] - ETA: 2:09 - loss: 0.8542 - accuracy: 0.7213\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            " 99/136 [====================>.........] - ETA: 2:05 - loss: 0.8542 - accuracy: 0.7214\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "100/136 [=====================>........] - ETA: 2:02 - loss: 0.8543 - accuracy: 0.7215\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "101/136 [=====================>........] - ETA: 1:59 - loss: 0.8543 - accuracy: 0.7216\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "102/136 [=====================>........] - ETA: 1:55 - loss: 0.8543 - accuracy: 0.7217\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "103/136 [=====================>........] - ETA: 1:52 - loss: 0.8542 - accuracy: 0.7218\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "104/136 [=====================>........] - ETA: 1:48 - loss: 0.8542 - accuracy: 0.7219\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "105/136 [======================>.......] - ETA: 1:45 - loss: 0.8541 - accuracy: 0.7221\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "106/136 [======================>.......] - ETA: 1:42 - loss: 0.8540 - accuracy: 0.7222\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "107/136 [======================>.......] - ETA: 1:38 - loss: 0.8538 - accuracy: 0.7224\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "108/136 [======================>.......] - ETA: 1:35 - loss: 0.8536 - accuracy: 0.7226\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "109/136 [=======================>......] - ETA: 1:32 - loss: 0.8534 - accuracy: 0.7227\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "110/136 [=======================>......] - ETA: 1:28 - loss: 0.8532 - accuracy: 0.7229\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "111/136 [=======================>......] - ETA: 1:25 - loss: 0.8530 - accuracy: 0.7231\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "112/136 [=======================>......] - ETA: 1:21 - loss: 0.8527 - accuracy: 0.7233\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "113/136 [=======================>......] - ETA: 1:18 - loss: 0.8524 - accuracy: 0.7235\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "114/136 [========================>.....] - ETA: 1:15 - loss: 0.8522 - accuracy: 0.7236\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "115/136 [========================>.....] - ETA: 1:11 - loss: 0.8520 - accuracy: 0.7238\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "116/136 [========================>.....] - ETA: 1:08 - loss: 0.8518 - accuracy: 0.7240\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "117/136 [========================>.....] - ETA: 1:04 - loss: 0.8516 - accuracy: 0.7241\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "118/136 [=========================>....] - ETA: 1:01 - loss: 0.8514 - accuracy: 0.7243\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "119/136 [=========================>....] - ETA: 57s - loss: 0.8512 - accuracy: 0.7244 \n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "120/136 [=========================>....] - ETA: 54s - loss: 0.8510 - accuracy: 0.7245\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "121/136 [=========================>....] - ETA: 51s - loss: 0.8508 - accuracy: 0.7247\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "122/136 [=========================>....] - ETA: 47s - loss: 0.8506 - accuracy: 0.7248\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "123/136 [==========================>...] - ETA: 44s - loss: 0.8504 - accuracy: 0.7250\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "124/136 [==========================>...] - ETA: 41s - loss: 0.8502 - accuracy: 0.7251\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "125/136 [==========================>...] - ETA: 37s - loss: 0.8500 - accuracy: 0.7253\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "126/136 [==========================>...] - ETA: 34s - loss: 0.8498 - accuracy: 0.7254\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "127/136 [===========================>..] - ETA: 30s - loss: 0.8496 - accuracy: 0.7255\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "128/136 [===========================>..] - ETA: 27s - loss: 0.8494 - accuracy: 0.7257\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "129/136 [===========================>..] - ETA: 23s - loss: 0.8492 - accuracy: 0.7258\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "130/136 [===========================>..] - ETA: 20s - loss: 0.8491 - accuracy: 0.7259\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "131/136 [===========================>..] - ETA: 17s - loss: 0.8489 - accuracy: 0.7260\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "132/136 [============================>.] - ETA: 13s - loss: 0.8488 - accuracy: 0.7261\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "133/136 [============================>.] - ETA: 10s - loss: 0.8486 - accuracy: 0.7262\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "134/136 [============================>.] - ETA: 6s - loss: 0.8485 - accuracy: 0.7263 \n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "135/136 [============================>.] - ETA: 3s - loss: 0.8484 - accuracy: 0.7263\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "136/136 [==============================] - ETA: 0s - loss: 0.8483 - accuracy: 0.7264\n",
            "Epoch 00005: loss did not improve from 0.52112\n",
            "136/136 [==============================] - 511s 4s/step - loss: 0.8481 - accuracy: 0.7265 - val_loss: 0.9106 - val_accuracy: 0.7875\n",
            "Epoch 6/10\n",
            "  1/136 [..............................] - ETA: 11:38 - loss: 0.8657 - accuracy: 0.7000\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "  2/136 [..............................] - ETA: 8:45 - loss: 1.0249 - accuracy: 0.7000 \n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "  3/136 [..............................] - ETA: 8:33 - loss: 0.9860 - accuracy: 0.7222\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "  4/136 [..............................] - ETA: 8:28 - loss: 0.9313 - accuracy: 0.7479\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "  5/136 [>.............................] - ETA: 8:28 - loss: 1.0211 - accuracy: 0.7623\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "  6/136 [>.............................] - ETA: 8:10 - loss: 1.0552 - accuracy: 0.7714\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "  7/136 [>.............................] - ETA: 7:58 - loss: 1.0652 - accuracy: 0.7796\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "  8/136 [>.............................] - ETA: 7:46 - loss: 1.0619 - accuracy: 0.7868\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "  9/136 [>.............................] - ETA: 7:37 - loss: 1.0566 - accuracy: 0.7920\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 10/136 [=>............................] - ETA: 7:30 - loss: 1.0489 - accuracy: 0.7968\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 11/136 [=>............................] - ETA: 7:24 - loss: 1.0412 - accuracy: 0.7995\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 12/136 [=>............................] - ETA: 7:20 - loss: 1.0336 - accuracy: 0.8017\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 13/136 [=>............................] - ETA: 7:42 - loss: 1.0268 - accuracy: 0.8027\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 14/136 [==>...........................] - ETA: 7:45 - loss: 1.0180 - accuracy: 0.8041\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 15/136 [==>...........................] - ETA: 7:39 - loss: 1.0113 - accuracy: 0.8042\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 16/136 [==>...........................] - ETA: 7:33 - loss: 1.0051 - accuracy: 0.8036\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 17/136 [==>...........................] - ETA: 7:27 - loss: 0.9994 - accuracy: 0.8027\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 18/136 [==>...........................] - ETA: 7:22 - loss: 0.9930 - accuracy: 0.8022\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 19/136 [===>..........................] - ETA: 7:15 - loss: 0.9872 - accuracy: 0.8015\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 20/136 [===>..........................] - ETA: 7:10 - loss: 0.9827 - accuracy: 0.8005\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 21/136 [===>..........................] - ETA: 7:05 - loss: 0.9787 - accuracy: 0.7995\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 22/136 [===>..........................] - ETA: 6:59 - loss: 0.9763 - accuracy: 0.7985\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 23/136 [====>.........................] - ETA: 6:54 - loss: 0.9736 - accuracy: 0.7976\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 24/136 [====>.........................] - ETA: 6:49 - loss: 0.9719 - accuracy: 0.7964\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 25/136 [====>.........................] - ETA: 6:44 - loss: 0.9695 - accuracy: 0.7954\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 26/136 [====>.........................] - ETA: 6:39 - loss: 0.9667 - accuracy: 0.7947\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 27/136 [====>.........................] - ETA: 6:34 - loss: 0.9639 - accuracy: 0.7939\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 28/136 [=====>........................] - ETA: 6:29 - loss: 0.9612 - accuracy: 0.7931\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 29/136 [=====>........................] - ETA: 6:25 - loss: 0.9587 - accuracy: 0.7924\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 30/136 [=====>........................] - ETA: 6:21 - loss: 0.9559 - accuracy: 0.7919\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 31/136 [=====>........................] - ETA: 6:17 - loss: 0.9531 - accuracy: 0.7914\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 32/136 [======>.......................] - ETA: 6:13 - loss: 0.9507 - accuracy: 0.7909\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 33/136 [======>.......................] - ETA: 6:09 - loss: 0.9479 - accuracy: 0.7905\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 34/136 [======>.......................] - ETA: 6:05 - loss: 0.9449 - accuracy: 0.7903\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 35/136 [======>.......................] - ETA: 6:00 - loss: 0.9417 - accuracy: 0.7902\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 36/136 [======>.......................] - ETA: 5:55 - loss: 0.9386 - accuracy: 0.7900\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 37/136 [=======>......................] - ETA: 5:51 - loss: 0.9355 - accuracy: 0.7899\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 38/136 [=======>......................] - ETA: 5:47 - loss: 0.9322 - accuracy: 0.7899\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 39/136 [=======>......................] - ETA: 5:43 - loss: 0.9289 - accuracy: 0.7899\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 40/136 [=======>......................] - ETA: 5:39 - loss: 0.9258 - accuracy: 0.7899\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 41/136 [========>.....................] - ETA: 5:36 - loss: 0.9225 - accuracy: 0.7899\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 42/136 [========>.....................] - ETA: 5:33 - loss: 0.9191 - accuracy: 0.7900\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 43/136 [========>.....................] - ETA: 5:29 - loss: 0.9159 - accuracy: 0.7901\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 44/136 [========>.....................] - ETA: 5:25 - loss: 0.9126 - accuracy: 0.7901\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 45/136 [========>.....................] - ETA: 5:20 - loss: 0.9095 - accuracy: 0.7902\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 46/136 [=========>....................] - ETA: 5:16 - loss: 0.9065 - accuracy: 0.7902\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 47/136 [=========>....................] - ETA: 5:12 - loss: 0.9035 - accuracy: 0.7903\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 48/136 [=========>....................] - ETA: 5:04 - loss: 0.9006 - accuracy: 0.7904\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 49/136 [=========>....................] - ETA: 5:02 - loss: 0.8977 - accuracy: 0.7905\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 50/136 [==========>...................] - ETA: 4:58 - loss: 0.8953 - accuracy: 0.7906\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 51/136 [==========>...................] - ETA: 4:54 - loss: 0.8928 - accuracy: 0.7907\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 52/136 [==========>...................] - ETA: 4:51 - loss: 0.8903 - accuracy: 0.7908\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 53/136 [==========>...................] - ETA: 4:48 - loss: 0.8877 - accuracy: 0.7909\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 54/136 [==========>...................] - ETA: 4:45 - loss: 0.8852 - accuracy: 0.7910\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 55/136 [===========>..................] - ETA: 4:41 - loss: 0.8829 - accuracy: 0.7911\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 56/136 [===========>..................] - ETA: 4:38 - loss: 0.8805 - accuracy: 0.7912\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 57/136 [===========>..................] - ETA: 4:35 - loss: 0.8787 - accuracy: 0.7911\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 58/136 [===========>..................] - ETA: 4:31 - loss: 0.8770 - accuracy: 0.7910\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 59/136 [============>.................] - ETA: 4:27 - loss: 0.8752 - accuracy: 0.7910\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 60/136 [============>.................] - ETA: 4:24 - loss: 0.8738 - accuracy: 0.7910\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 61/136 [============>.................] - ETA: 4:20 - loss: 0.8724 - accuracy: 0.7910\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 62/136 [============>.................] - ETA: 4:17 - loss: 0.8710 - accuracy: 0.7910\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 63/136 [============>.................] - ETA: 4:14 - loss: 0.8696 - accuracy: 0.7910\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 64/136 [=============>................] - ETA: 4:10 - loss: 0.8682 - accuracy: 0.7911\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 65/136 [=============>................] - ETA: 4:06 - loss: 0.8668 - accuracy: 0.7911\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 66/136 [=============>................] - ETA: 4:03 - loss: 0.8654 - accuracy: 0.7911\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 67/136 [=============>................] - ETA: 3:59 - loss: 0.8644 - accuracy: 0.7911\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 68/136 [==============>...............] - ETA: 3:55 - loss: 0.8635 - accuracy: 0.7911\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 69/136 [==============>...............] - ETA: 3:52 - loss: 0.8626 - accuracy: 0.7910\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 70/136 [==============>...............] - ETA: 3:49 - loss: 0.8616 - accuracy: 0.7911\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 71/136 [==============>...............] - ETA: 3:45 - loss: 0.8607 - accuracy: 0.7911\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 72/136 [==============>...............] - ETA: 3:42 - loss: 0.8598 - accuracy: 0.7910\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 73/136 [===============>..............] - ETA: 3:38 - loss: 0.8589 - accuracy: 0.7910\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 74/136 [===============>..............] - ETA: 3:35 - loss: 0.8580 - accuracy: 0.7910\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 75/136 [===============>..............] - ETA: 3:31 - loss: 0.8571 - accuracy: 0.7910\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 76/136 [===============>..............] - ETA: 3:28 - loss: 0.8561 - accuracy: 0.7910\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 77/136 [===============>..............] - ETA: 3:24 - loss: 0.8552 - accuracy: 0.7910\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 78/136 [================>.............] - ETA: 3:21 - loss: 0.8543 - accuracy: 0.7911\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 79/136 [================>.............] - ETA: 3:17 - loss: 0.8533 - accuracy: 0.7911\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 80/136 [================>.............] - ETA: 3:14 - loss: 0.8523 - accuracy: 0.7912\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 81/136 [================>.............] - ETA: 3:10 - loss: 0.8512 - accuracy: 0.7913\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 82/136 [=================>............] - ETA: 3:07 - loss: 0.8502 - accuracy: 0.7913\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 83/136 [=================>............] - ETA: 3:03 - loss: 0.8492 - accuracy: 0.7914\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 84/136 [=================>............] - ETA: 3:00 - loss: 0.8483 - accuracy: 0.7915\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 85/136 [=================>............] - ETA: 2:56 - loss: 0.8474 - accuracy: 0.7915\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 86/136 [=================>............] - ETA: 2:53 - loss: 0.8465 - accuracy: 0.7915\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 87/136 [==================>...........] - ETA: 2:49 - loss: 0.8456 - accuracy: 0.7916\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 88/136 [==================>...........] - ETA: 2:46 - loss: 0.8448 - accuracy: 0.7915\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 89/136 [==================>...........] - ETA: 2:42 - loss: 0.8440 - accuracy: 0.7915\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 90/136 [==================>...........] - ETA: 2:39 - loss: 0.8433 - accuracy: 0.7914\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 91/136 [===================>..........] - ETA: 2:35 - loss: 0.8426 - accuracy: 0.7913\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 92/136 [===================>..........] - ETA: 2:32 - loss: 0.8419 - accuracy: 0.7913\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 93/136 [===================>..........] - ETA: 2:28 - loss: 0.8412 - accuracy: 0.7912\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 94/136 [===================>..........] - ETA: 2:25 - loss: 0.8405 - accuracy: 0.7911\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 95/136 [===================>..........] - ETA: 2:21 - loss: 0.8398 - accuracy: 0.7911\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 96/136 [====================>.........] - ETA: 2:18 - loss: 0.8390 - accuracy: 0.7910\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 97/136 [====================>.........] - ETA: 2:14 - loss: 0.8383 - accuracy: 0.7910\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 98/136 [====================>.........] - ETA: 2:11 - loss: 0.8376 - accuracy: 0.7909\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            " 99/136 [====================>.........] - ETA: 2:07 - loss: 0.8369 - accuracy: 0.7909\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "100/136 [=====================>........] - ETA: 2:04 - loss: 0.8364 - accuracy: 0.7908\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "101/136 [=====================>........] - ETA: 2:00 - loss: 0.8357 - accuracy: 0.7908\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "102/136 [=====================>........] - ETA: 1:57 - loss: 0.8351 - accuracy: 0.7907\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "103/136 [=====================>........] - ETA: 1:53 - loss: 0.8344 - accuracy: 0.7907\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "104/136 [=====================>........] - ETA: 1:50 - loss: 0.8337 - accuracy: 0.7907\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "105/136 [======================>.......] - ETA: 1:46 - loss: 0.8330 - accuracy: 0.7907\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "106/136 [======================>.......] - ETA: 1:43 - loss: 0.8324 - accuracy: 0.7906\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "107/136 [======================>.......] - ETA: 1:40 - loss: 0.8321 - accuracy: 0.7906\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "108/136 [======================>.......] - ETA: 1:36 - loss: 0.8318 - accuracy: 0.7905\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "109/136 [=======================>......] - ETA: 1:33 - loss: 0.8316 - accuracy: 0.7905\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "110/136 [=======================>......] - ETA: 1:29 - loss: 0.8313 - accuracy: 0.7904\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "111/136 [=======================>......] - ETA: 1:26 - loss: 0.8310 - accuracy: 0.7904\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "112/136 [=======================>......] - ETA: 1:22 - loss: 0.8308 - accuracy: 0.7903\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "113/136 [=======================>......] - ETA: 1:19 - loss: 0.8305 - accuracy: 0.7903\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "114/136 [========================>.....] - ETA: 1:15 - loss: 0.8302 - accuracy: 0.7902\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "115/136 [========================>.....] - ETA: 1:12 - loss: 0.8300 - accuracy: 0.7902\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "116/136 [========================>.....] - ETA: 1:08 - loss: 0.8297 - accuracy: 0.7901\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "117/136 [========================>.....] - ETA: 1:05 - loss: 0.8295 - accuracy: 0.7901\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "118/136 [=========================>....] - ETA: 1:02 - loss: 0.8292 - accuracy: 0.7900\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "119/136 [=========================>....] - ETA: 58s - loss: 0.8290 - accuracy: 0.7900 \n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "120/136 [=========================>....] - ETA: 55s - loss: 0.8288 - accuracy: 0.7899\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "121/136 [=========================>....] - ETA: 51s - loss: 0.8286 - accuracy: 0.7898\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "122/136 [=========================>....] - ETA: 48s - loss: 0.8284 - accuracy: 0.7898\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "123/136 [==========================>...] - ETA: 44s - loss: 0.8282 - accuracy: 0.7897\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "124/136 [==========================>...] - ETA: 41s - loss: 0.8280 - accuracy: 0.7897\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "125/136 [==========================>...] - ETA: 37s - loss: 0.8278 - accuracy: 0.7896\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "126/136 [==========================>...] - ETA: 34s - loss: 0.8277 - accuracy: 0.7895\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "127/136 [===========================>..] - ETA: 30s - loss: 0.8275 - accuracy: 0.7894\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "128/136 [===========================>..] - ETA: 27s - loss: 0.8273 - accuracy: 0.7894\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "129/136 [===========================>..] - ETA: 24s - loss: 0.8271 - accuracy: 0.7893\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "130/136 [===========================>..] - ETA: 20s - loss: 0.8269 - accuracy: 0.7893\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "131/136 [===========================>..] - ETA: 17s - loss: 0.8266 - accuracy: 0.7892\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "132/136 [============================>.] - ETA: 13s - loss: 0.8264 - accuracy: 0.7892\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "133/136 [============================>.] - ETA: 10s - loss: 0.8261 - accuracy: 0.7891\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "134/136 [============================>.] - ETA: 6s - loss: 0.8259 - accuracy: 0.7891 \n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "135/136 [============================>.] - ETA: 3s - loss: 0.8256 - accuracy: 0.7890\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "136/136 [==============================] - ETA: 0s - loss: 0.8253 - accuracy: 0.7890\n",
            "Epoch 00006: loss did not improve from 0.52112\n",
            "136/136 [==============================] - 513s 4s/step - loss: 0.8250 - accuracy: 0.7890 - val_loss: 0.8121 - val_accuracy: 0.8000\n",
            "Epoch 7/10\n",
            "  1/136 [..............................] - ETA: 13:23 - loss: 0.9814 - accuracy: 0.7000\n",
            "Epoch 00007: loss did not improve from 0.52112\n",
            "  2/136 [..............................] - ETA: 8:45 - loss: 0.7598 - accuracy: 0.7750 \n",
            "Epoch 00007: loss did not improve from 0.52112\n",
            "  3/136 [..............................] - ETA: 8:36 - loss: 0.6788 - accuracy: 0.8056\n",
            "Epoch 00007: loss improved from 0.52112 to 0.51689, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "  4/136 [..............................] - ETA: 11:14 - loss: 0.6331 - accuracy: 0.8229\n",
            "Epoch 00007: loss improved from 0.51689 to 0.49599, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "  5/136 [>.............................] - ETA: 10:41 - loss: 0.5910 - accuracy: 0.8383\n",
            "Epoch 00007: loss improved from 0.49599 to 0.42243, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "  6/136 [>.............................] - ETA: 10:18 - loss: 0.5558 - accuracy: 0.8514\n",
            "Epoch 00007: loss improved from 0.42243 to 0.38019, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "  7/136 [>.............................] - ETA: 10:03 - loss: 0.5313 - accuracy: 0.8604\n",
            "Epoch 00007: loss did not improve from 0.38019\n",
            "  8/136 [>.............................] - ETA: 9:35 - loss: 0.5095 - accuracy: 0.8685 \n",
            "Epoch 00007: loss improved from 0.38019 to 0.35651, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            "  9/136 [>.............................] - ETA: 9:25 - loss: 0.4892 - accuracy: 0.8757\n",
            "Epoch 00007: loss improved from 0.35651 to 0.32674, saving model to /content/gdrive/My Drive/Colab Notebooks/classifier_dataset_v4/best_model_xs_v4.h5\n",
            " 10/136 [=>............................] - ETA: 9:15 - loss: 0.4817 - accuracy: 0.8781\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 11/136 [=>............................] - ETA: 9:02 - loss: 0.4777 - accuracy: 0.8793\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 12/136 [=>............................] - ETA: 8:53 - loss: 0.4739 - accuracy: 0.8803\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 13/136 [=>............................] - ETA: 8:38 - loss: 0.4702 - accuracy: 0.8812\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 14/136 [==>...........................] - ETA: 8:27 - loss: 0.4666 - accuracy: 0.8820\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 15/136 [==>...........................] - ETA: 8:14 - loss: 0.4633 - accuracy: 0.8828\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 16/136 [==>...........................] - ETA: 8:06 - loss: 0.4624 - accuracy: 0.8827\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 17/136 [==>...........................] - ETA: 7:59 - loss: 0.4605 - accuracy: 0.8830\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 18/136 [==>...........................] - ETA: 7:50 - loss: 0.4587 - accuracy: 0.8834\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 19/136 [===>..........................] - ETA: 7:42 - loss: 0.4570 - accuracy: 0.8837\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 20/136 [===>..........................] - ETA: 7:35 - loss: 0.4545 - accuracy: 0.8842\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 21/136 [===>..........................] - ETA: 7:28 - loss: 0.4531 - accuracy: 0.8848\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 22/136 [===>..........................] - ETA: 7:22 - loss: 0.4517 - accuracy: 0.8853\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 23/136 [====>.........................] - ETA: 7:16 - loss: 0.4501 - accuracy: 0.8857\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 24/136 [====>.........................] - ETA: 7:12 - loss: 0.4491 - accuracy: 0.8860\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 25/136 [====>.........................] - ETA: 7:25 - loss: 0.4486 - accuracy: 0.8860\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 26/136 [====>.........................] - ETA: 7:19 - loss: 0.4486 - accuracy: 0.8860\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 27/136 [====>.........................] - ETA: 7:12 - loss: 0.4484 - accuracy: 0.8860\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 28/136 [=====>........................] - ETA: 7:06 - loss: 0.4484 - accuracy: 0.8858\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 29/136 [=====>........................] - ETA: 7:00 - loss: 0.4485 - accuracy: 0.8857\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 30/136 [=====>........................] - ETA: 6:55 - loss: 0.4490 - accuracy: 0.8854\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 31/136 [=====>........................] - ETA: 6:49 - loss: 0.4494 - accuracy: 0.8852\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 32/136 [======>.......................] - ETA: 6:45 - loss: 0.4498 - accuracy: 0.8849\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 33/136 [======>.......................] - ETA: 6:39 - loss: 0.4501 - accuracy: 0.8847\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 34/136 [======>.......................] - ETA: 6:34 - loss: 0.4505 - accuracy: 0.8845\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 35/136 [======>.......................] - ETA: 6:28 - loss: 0.4510 - accuracy: 0.8842\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 36/136 [======>.......................] - ETA: 6:23 - loss: 0.4519 - accuracy: 0.8838\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 37/136 [=======>......................] - ETA: 6:17 - loss: 0.4529 - accuracy: 0.8834\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 38/136 [=======>......................] - ETA: 6:13 - loss: 0.4541 - accuracy: 0.8828\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 39/136 [=======>......................] - ETA: 6:08 - loss: 0.4556 - accuracy: 0.8822\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 40/136 [=======>......................] - ETA: 6:04 - loss: 0.4567 - accuracy: 0.8817\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 41/136 [========>.....................] - ETA: 5:59 - loss: 0.4580 - accuracy: 0.8811\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 42/136 [========>.....................] - ETA: 5:54 - loss: 0.4592 - accuracy: 0.8806\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 43/136 [========>.....................] - ETA: 5:50 - loss: 0.4608 - accuracy: 0.8800\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 44/136 [========>.....................] - ETA: 5:45 - loss: 0.4624 - accuracy: 0.8793\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 45/136 [========>.....................] - ETA: 5:41 - loss: 0.4639 - accuracy: 0.8786\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 46/136 [=========>....................] - ETA: 5:36 - loss: 0.4655 - accuracy: 0.8780\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 47/136 [=========>....................] - ETA: 5:33 - loss: 0.4673 - accuracy: 0.8772\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 48/136 [=========>....................] - ETA: 5:28 - loss: 0.4688 - accuracy: 0.8765\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 49/136 [=========>....................] - ETA: 5:25 - loss: 0.4702 - accuracy: 0.8759\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 50/136 [==========>...................] - ETA: 5:21 - loss: 0.4716 - accuracy: 0.8752\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 51/136 [==========>...................] - ETA: 5:16 - loss: 0.4727 - accuracy: 0.8747\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 52/136 [==========>...................] - ETA: 5:13 - loss: 0.4740 - accuracy: 0.8741\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 53/136 [==========>...................] - ETA: 5:09 - loss: 0.4764 - accuracy: 0.8735\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 54/136 [==========>...................] - ETA: 5:05 - loss: 0.4787 - accuracy: 0.8728\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 55/136 [===========>..................] - ETA: 5:01 - loss: 0.4809 - accuracy: 0.8722\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 56/136 [===========>..................] - ETA: 4:57 - loss: 0.4831 - accuracy: 0.8716\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 57/136 [===========>..................] - ETA: 4:53 - loss: 0.4853 - accuracy: 0.8709\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 58/136 [===========>..................] - ETA: 4:49 - loss: 0.4906 - accuracy: 0.8703\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 59/136 [============>.................] - ETA: 4:45 - loss: 0.4955 - accuracy: 0.8697\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 60/136 [============>.................] - ETA: 4:41 - loss: 0.5002 - accuracy: 0.8692\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 61/136 [============>.................] - ETA: 4:37 - loss: 0.5045 - accuracy: 0.8687\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 62/136 [============>.................] - ETA: 4:33 - loss: 0.5086 - accuracy: 0.8683\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 63/136 [============>.................] - ETA: 4:29 - loss: 0.5125 - accuracy: 0.8678\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 64/136 [=============>................] - ETA: 4:25 - loss: 0.5161 - accuracy: 0.8674\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 65/136 [=============>................] - ETA: 4:21 - loss: 0.5197 - accuracy: 0.8670\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 66/136 [=============>................] - ETA: 4:17 - loss: 0.5231 - accuracy: 0.8666\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 67/136 [=============>................] - ETA: 4:13 - loss: 0.5264 - accuracy: 0.8662\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 68/136 [==============>...............] - ETA: 4:09 - loss: 0.5295 - accuracy: 0.8658\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 69/136 [==============>...............] - ETA: 4:05 - loss: 0.5325 - accuracy: 0.8654\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 70/136 [==============>...............] - ETA: 4:01 - loss: 0.5354 - accuracy: 0.8650\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 71/136 [==============>...............] - ETA: 3:57 - loss: 0.5380 - accuracy: 0.8647\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 72/136 [==============>...............] - ETA: 3:53 - loss: 0.5407 - accuracy: 0.8644\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 73/136 [===============>..............] - ETA: 3:50 - loss: 0.5431 - accuracy: 0.8640\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 74/136 [===============>..............] - ETA: 3:46 - loss: 0.5456 - accuracy: 0.8637\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 75/136 [===============>..............] - ETA: 3:42 - loss: 0.5479 - accuracy: 0.8633\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 76/136 [===============>..............] - ETA: 3:38 - loss: 0.5502 - accuracy: 0.8630\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 77/136 [===============>..............] - ETA: 3:34 - loss: 0.5523 - accuracy: 0.8627\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 78/136 [================>.............] - ETA: 3:30 - loss: 0.5543 - accuracy: 0.8624\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 79/136 [================>.............] - ETA: 3:27 - loss: 0.5561 - accuracy: 0.8622\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 80/136 [================>.............] - ETA: 3:23 - loss: 0.5579 - accuracy: 0.8619\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 81/136 [================>.............] - ETA: 3:19 - loss: 0.5595 - accuracy: 0.8617\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 82/136 [=================>............] - ETA: 3:15 - loss: 0.5611 - accuracy: 0.8615\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 83/136 [=================>............] - ETA: 3:12 - loss: 0.5626 - accuracy: 0.8613\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 84/136 [=================>............] - ETA: 3:08 - loss: 0.5641 - accuracy: 0.8611\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 85/136 [=================>............] - ETA: 3:04 - loss: 0.5654 - accuracy: 0.8609\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 86/136 [=================>............] - ETA: 3:00 - loss: 0.5669 - accuracy: 0.8608\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 87/136 [==================>...........] - ETA: 2:57 - loss: 0.5684 - accuracy: 0.8606\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 88/136 [==================>...........] - ETA: 2:53 - loss: 0.5699 - accuracy: 0.8604\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 89/136 [==================>...........] - ETA: 2:50 - loss: 0.5713 - accuracy: 0.8603\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 90/136 [==================>...........] - ETA: 2:46 - loss: 0.5726 - accuracy: 0.8601\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 91/136 [===================>..........] - ETA: 2:42 - loss: 0.5739 - accuracy: 0.8600\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 92/136 [===================>..........] - ETA: 2:38 - loss: 0.5750 - accuracy: 0.8599\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 93/136 [===================>..........] - ETA: 2:35 - loss: 0.5761 - accuracy: 0.8598\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 94/136 [===================>..........] - ETA: 2:31 - loss: 0.5774 - accuracy: 0.8597\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 95/136 [===================>..........] - ETA: 2:27 - loss: 0.5787 - accuracy: 0.8595\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 96/136 [====================>.........] - ETA: 2:24 - loss: 0.5799 - accuracy: 0.8594\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 97/136 [====================>.........] - ETA: 2:20 - loss: 0.5811 - accuracy: 0.8593\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 98/136 [====================>.........] - ETA: 2:16 - loss: 0.5822 - accuracy: 0.8592\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            " 99/136 [====================>.........] - ETA: 2:13 - loss: 0.5833 - accuracy: 0.8591\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "100/136 [=====================>........] - ETA: 2:09 - loss: 0.5843 - accuracy: 0.8590\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "101/136 [=====================>........] - ETA: 2:05 - loss: 0.5854 - accuracy: 0.8589\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "102/136 [=====================>........] - ETA: 2:02 - loss: 0.5864 - accuracy: 0.8587\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "103/136 [=====================>........] - ETA: 1:58 - loss: 0.5874 - accuracy: 0.8586\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "104/136 [=====================>........] - ETA: 1:54 - loss: 0.5884 - accuracy: 0.8585\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "105/136 [======================>.......] - ETA: 1:51 - loss: 0.5892 - accuracy: 0.8584\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "106/136 [======================>.......] - ETA: 1:47 - loss: 0.5901 - accuracy: 0.8583\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "107/136 [======================>.......] - ETA: 1:43 - loss: 0.5909 - accuracy: 0.8582\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "108/136 [======================>.......] - ETA: 1:40 - loss: 0.5917 - accuracy: 0.8581\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "109/136 [=======================>......] - ETA: 1:36 - loss: 0.5925 - accuracy: 0.8580\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "110/136 [=======================>......] - ETA: 1:33 - loss: 0.5933 - accuracy: 0.8579\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "111/136 [=======================>......] - ETA: 1:29 - loss: 0.5941 - accuracy: 0.8578\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "112/136 [=======================>......] - ETA: 1:25 - loss: 0.5949 - accuracy: 0.8577\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "113/136 [=======================>......] - ETA: 1:22 - loss: 0.5957 - accuracy: 0.8575\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "114/136 [========================>.....] - ETA: 1:18 - loss: 0.5964 - accuracy: 0.8574\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "115/136 [========================>.....] - ETA: 1:14 - loss: 0.5971 - accuracy: 0.8573\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "116/136 [========================>.....] - ETA: 1:11 - loss: 0.5982 - accuracy: 0.8571\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "117/136 [========================>.....] - ETA: 1:07 - loss: 0.5992 - accuracy: 0.8570\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "118/136 [=========================>....] - ETA: 1:04 - loss: 0.6003 - accuracy: 0.8569\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "119/136 [=========================>....] - ETA: 1:00 - loss: 0.6013 - accuracy: 0.8567\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "120/136 [=========================>....] - ETA: 56s - loss: 0.6023 - accuracy: 0.8566 \n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "121/136 [=========================>....] - ETA: 52s - loss: 0.6032 - accuracy: 0.8564\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "122/136 [=========================>....] - ETA: 49s - loss: 0.6042 - accuracy: 0.8562\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "123/136 [==========================>...] - ETA: 45s - loss: 0.6052 - accuracy: 0.8561\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "124/136 [==========================>...] - ETA: 42s - loss: 0.6062 - accuracy: 0.8559\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "125/136 [==========================>...] - ETA: 38s - loss: 0.6072 - accuracy: 0.8557\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "126/136 [==========================>...] - ETA: 35s - loss: 0.6081 - accuracy: 0.8556\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "127/136 [===========================>..] - ETA: 31s - loss: 0.6091 - accuracy: 0.8554\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "128/136 [===========================>..] - ETA: 28s - loss: 0.6100 - accuracy: 0.8553\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "129/136 [===========================>..] - ETA: 24s - loss: 0.6109 - accuracy: 0.8551\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "130/136 [===========================>..] - ETA: 21s - loss: 0.6118 - accuracy: 0.8550\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "131/136 [===========================>..] - ETA: 17s - loss: 0.6128 - accuracy: 0.8548\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "132/136 [============================>.] - ETA: 14s - loss: 0.6137 - accuracy: 0.8546\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "133/136 [============================>.] - ETA: 10s - loss: 0.6146 - accuracy: 0.8544\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "134/136 [============================>.] - ETA: 7s - loss: 0.6155 - accuracy: 0.8543 \n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "135/136 [============================>.] - ETA: 3s - loss: 0.6164 - accuracy: 0.8541\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "136/136 [==============================] - ETA: 0s - loss: 0.6173 - accuracy: 0.8539\n",
            "Epoch 00007: loss did not improve from 0.32674\n",
            "136/136 [==============================] - 524s 4s/step - loss: 0.6182 - accuracy: 0.8537 - val_loss: 0.7206 - val_accuracy: 0.8417\n",
            "Epoch 8/10\n",
            "  1/136 [..............................] - ETA: 11:34 - loss: 0.3926 - accuracy: 0.9000\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "  2/136 [..............................] - ETA: 8:36 - loss: 0.3966 - accuracy: 0.8750 \n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "  3/136 [..............................] - ETA: 8:39 - loss: 0.4370 - accuracy: 0.8611\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "  4/136 [..............................] - ETA: 8:29 - loss: 0.4472 - accuracy: 0.8583\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "  5/136 [>.............................] - ETA: 8:08 - loss: 0.4570 - accuracy: 0.8547\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "  6/136 [>.............................] - ETA: 8:01 - loss: 0.4628 - accuracy: 0.8511\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "  7/136 [>.............................] - ETA: 7:50 - loss: 0.4649 - accuracy: 0.8479\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "  8/136 [>.............................] - ETA: 7:41 - loss: 0.4736 - accuracy: 0.8419\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "  9/136 [>.............................] - ETA: 7:34 - loss: 0.4998 - accuracy: 0.8372\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 10/136 [=>............................] - ETA: 7:29 - loss: 0.5203 - accuracy: 0.8335\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 11/136 [=>............................] - ETA: 7:24 - loss: 0.5411 - accuracy: 0.8296\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 12/136 [=>............................] - ETA: 7:19 - loss: 0.5551 - accuracy: 0.8279\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 13/136 [=>............................] - ETA: 7:13 - loss: 0.5649 - accuracy: 0.8269\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 14/136 [==>...........................] - ETA: 7:07 - loss: 0.5728 - accuracy: 0.8260\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 15/136 [==>...........................] - ETA: 7:04 - loss: 0.5784 - accuracy: 0.8256\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 16/136 [==>...........................] - ETA: 7:00 - loss: 0.5814 - accuracy: 0.8260\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 17/136 [==>...........................] - ETA: 6:56 - loss: 0.5858 - accuracy: 0.8258\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 18/136 [==>...........................] - ETA: 6:52 - loss: 0.5886 - accuracy: 0.8259\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 19/136 [===>..........................] - ETA: 6:47 - loss: 0.5905 - accuracy: 0.8262\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 20/136 [===>..........................] - ETA: 6:43 - loss: 0.5914 - accuracy: 0.8267\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 21/136 [===>..........................] - ETA: 6:42 - loss: 0.5914 - accuracy: 0.8274\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 22/136 [===>..........................] - ETA: 6:39 - loss: 0.5905 - accuracy: 0.8285\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 23/136 [====>.........................] - ETA: 6:35 - loss: 0.5894 - accuracy: 0.8295\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 24/136 [====>.........................] - ETA: 6:31 - loss: 0.5877 - accuracy: 0.8307\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 25/136 [====>.........................] - ETA: 6:27 - loss: 0.5858 - accuracy: 0.8319\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 26/136 [====>.........................] - ETA: 6:24 - loss: 0.5882 - accuracy: 0.8330\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 27/136 [====>.........................] - ETA: 6:20 - loss: 0.5900 - accuracy: 0.8341\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 28/136 [=====>........................] - ETA: 6:17 - loss: 0.5917 - accuracy: 0.8351\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 29/136 [=====>........................] - ETA: 6:13 - loss: 0.5935 - accuracy: 0.8358\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 30/136 [=====>........................] - ETA: 6:09 - loss: 0.6033 - accuracy: 0.8363\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 31/136 [=====>........................] - ETA: 6:05 - loss: 0.6121 - accuracy: 0.8368\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 32/136 [======>.......................] - ETA: 6:01 - loss: 0.6254 - accuracy: 0.8370\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 33/136 [======>.......................] - ETA: 5:57 - loss: 0.6371 - accuracy: 0.8372\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 34/136 [======>.......................] - ETA: 5:54 - loss: 0.6481 - accuracy: 0.8374\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 35/136 [======>.......................] - ETA: 5:50 - loss: 0.6580 - accuracy: 0.8376\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 36/136 [======>.......................] - ETA: 5:46 - loss: 0.6672 - accuracy: 0.8376\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 37/136 [=======>......................] - ETA: 5:43 - loss: 0.6761 - accuracy: 0.8375\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 38/136 [=======>......................] - ETA: 5:37 - loss: 0.6848 - accuracy: 0.8372\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 39/136 [=======>......................] - ETA: 5:33 - loss: 0.6933 - accuracy: 0.8367\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 40/136 [=======>......................] - ETA: 5:30 - loss: 0.7039 - accuracy: 0.8361\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 41/136 [========>.....................] - ETA: 5:20 - loss: 0.7138 - accuracy: 0.8355\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 42/136 [========>.....................] - ETA: 5:18 - loss: 0.7239 - accuracy: 0.8349\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 43/136 [========>.....................] - ETA: 5:15 - loss: 0.7337 - accuracy: 0.8343\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 44/136 [========>.....................] - ETA: 5:11 - loss: 0.7428 - accuracy: 0.8335\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 45/136 [========>.....................] - ETA: 5:08 - loss: 0.7515 - accuracy: 0.8327\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 46/136 [=========>....................] - ETA: 5:05 - loss: 0.7600 - accuracy: 0.8318\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 47/136 [=========>....................] - ETA: 5:02 - loss: 0.7681 - accuracy: 0.8308\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 48/136 [=========>....................] - ETA: 4:58 - loss: 0.7759 - accuracy: 0.8297\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 49/136 [=========>....................] - ETA: 4:55 - loss: 0.7833 - accuracy: 0.8286\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 50/136 [==========>...................] - ETA: 4:52 - loss: 0.7904 - accuracy: 0.8275\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 51/136 [==========>...................] - ETA: 4:48 - loss: 0.7971 - accuracy: 0.8264\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 52/136 [==========>...................] - ETA: 4:45 - loss: 0.8034 - accuracy: 0.8253\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 53/136 [==========>...................] - ETA: 4:45 - loss: 0.8097 - accuracy: 0.8241\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 54/136 [==========>...................] - ETA: 4:44 - loss: 0.8156 - accuracy: 0.8230\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 55/136 [===========>..................] - ETA: 4:41 - loss: 0.8213 - accuracy: 0.8219\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 56/136 [===========>..................] - ETA: 4:37 - loss: 0.8266 - accuracy: 0.8208\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 57/136 [===========>..................] - ETA: 4:33 - loss: 0.8319 - accuracy: 0.8197\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 58/136 [===========>..................] - ETA: 4:30 - loss: 0.8379 - accuracy: 0.8185\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 59/136 [============>.................] - ETA: 4:26 - loss: 0.8438 - accuracy: 0.8174\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 60/136 [============>.................] - ETA: 4:23 - loss: 0.8492 - accuracy: 0.8163\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 61/136 [============>.................] - ETA: 4:19 - loss: 0.8545 - accuracy: 0.8152\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 62/136 [============>.................] - ETA: 4:16 - loss: 0.8594 - accuracy: 0.8142\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 63/136 [============>.................] - ETA: 4:12 - loss: 0.8641 - accuracy: 0.8132\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 64/136 [=============>................] - ETA: 4:09 - loss: 0.8685 - accuracy: 0.8122\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 65/136 [=============>................] - ETA: 4:05 - loss: 0.8727 - accuracy: 0.8112\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 66/136 [=============>................] - ETA: 4:02 - loss: 0.8768 - accuracy: 0.8103\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 67/136 [=============>................] - ETA: 3:59 - loss: 0.8808 - accuracy: 0.8094\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 68/136 [==============>...............] - ETA: 3:55 - loss: 0.8846 - accuracy: 0.8085\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 69/136 [==============>...............] - ETA: 3:52 - loss: 0.8884 - accuracy: 0.8075\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 70/136 [==============>...............] - ETA: 3:48 - loss: 0.8919 - accuracy: 0.8066\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 71/136 [==============>...............] - ETA: 3:45 - loss: 0.8954 - accuracy: 0.8057\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 72/136 [==============>...............] - ETA: 3:41 - loss: 0.8988 - accuracy: 0.8048\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 73/136 [===============>..............] - ETA: 3:38 - loss: 0.9021 - accuracy: 0.8039\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 74/136 [===============>..............] - ETA: 3:34 - loss: 0.9054 - accuracy: 0.8029\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 75/136 [===============>..............] - ETA: 3:31 - loss: 0.9085 - accuracy: 0.8020\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 76/136 [===============>..............] - ETA: 3:27 - loss: 0.9117 - accuracy: 0.8010\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 77/136 [===============>..............] - ETA: 3:24 - loss: 0.9157 - accuracy: 0.8000\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 78/136 [================>.............] - ETA: 3:20 - loss: 0.9196 - accuracy: 0.7991\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 79/136 [================>.............] - ETA: 3:17 - loss: 0.9234 - accuracy: 0.7981\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 80/136 [================>.............] - ETA: 3:13 - loss: 0.9272 - accuracy: 0.7971\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 81/136 [================>.............] - ETA: 3:10 - loss: 0.9308 - accuracy: 0.7962\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 82/136 [=================>............] - ETA: 3:06 - loss: 0.9344 - accuracy: 0.7952\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 83/136 [=================>............] - ETA: 3:03 - loss: 0.9378 - accuracy: 0.7943\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 84/136 [=================>............] - ETA: 2:59 - loss: 0.9411 - accuracy: 0.7934\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 85/136 [=================>............] - ETA: 2:56 - loss: 0.9443 - accuracy: 0.7925\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 86/136 [=================>............] - ETA: 2:52 - loss: 0.9474 - accuracy: 0.7916\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 87/136 [==================>...........] - ETA: 2:49 - loss: 0.9504 - accuracy: 0.7907\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 88/136 [==================>...........] - ETA: 2:45 - loss: 0.9533 - accuracy: 0.7899\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 89/136 [==================>...........] - ETA: 2:42 - loss: 0.9561 - accuracy: 0.7890\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 90/136 [==================>...........] - ETA: 2:38 - loss: 0.9588 - accuracy: 0.7882\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 91/136 [===================>..........] - ETA: 2:35 - loss: 0.9615 - accuracy: 0.7874\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 92/136 [===================>..........] - ETA: 2:32 - loss: 0.9640 - accuracy: 0.7866\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 93/136 [===================>..........] - ETA: 2:28 - loss: 0.9664 - accuracy: 0.7858\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 94/136 [===================>..........] - ETA: 2:25 - loss: 0.9688 - accuracy: 0.7851\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 95/136 [===================>..........] - ETA: 2:21 - loss: 0.9719 - accuracy: 0.7844\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 96/136 [====================>.........] - ETA: 2:18 - loss: 0.9750 - accuracy: 0.7836\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 97/136 [====================>.........] - ETA: 2:14 - loss: 0.9780 - accuracy: 0.7829\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 98/136 [====================>.........] - ETA: 2:11 - loss: 0.9808 - accuracy: 0.7822\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            " 99/136 [====================>.........] - ETA: 2:08 - loss: 0.9837 - accuracy: 0.7815\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "100/136 [=====================>........] - ETA: 2:04 - loss: 0.9864 - accuracy: 0.7809\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "101/136 [=====================>........] - ETA: 2:01 - loss: 0.9891 - accuracy: 0.7802\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "102/136 [=====================>........] - ETA: 1:57 - loss: 0.9916 - accuracy: 0.7795\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "103/136 [=====================>........] - ETA: 1:54 - loss: 0.9941 - accuracy: 0.7789\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "104/136 [=====================>........] - ETA: 1:50 - loss: 0.9965 - accuracy: 0.7782\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "105/136 [======================>.......] - ETA: 1:47 - loss: 0.9989 - accuracy: 0.7776\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "106/136 [======================>.......] - ETA: 1:43 - loss: 1.0011 - accuracy: 0.7769\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "107/136 [======================>.......] - ETA: 1:40 - loss: 1.0033 - accuracy: 0.7763\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "108/136 [======================>.......] - ETA: 1:36 - loss: 1.0054 - accuracy: 0.7756\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "109/136 [=======================>......] - ETA: 1:33 - loss: 1.0074 - accuracy: 0.7750\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "110/136 [=======================>......] - ETA: 1:29 - loss: 1.0094 - accuracy: 0.7744\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "111/136 [=======================>......] - ETA: 1:26 - loss: 1.0113 - accuracy: 0.7738\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "112/136 [=======================>......] - ETA: 1:22 - loss: 1.0132 - accuracy: 0.7732\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "113/136 [=======================>......] - ETA: 1:19 - loss: 1.0149 - accuracy: 0.7727\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "114/136 [========================>.....] - ETA: 1:16 - loss: 1.0167 - accuracy: 0.7721\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "115/136 [========================>.....] - ETA: 1:12 - loss: 1.0183 - accuracy: 0.7715\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "116/136 [========================>.....] - ETA: 1:09 - loss: 1.0200 - accuracy: 0.7710\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "117/136 [========================>.....] - ETA: 1:05 - loss: 1.0215 - accuracy: 0.7704\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "118/136 [=========================>....] - ETA: 1:02 - loss: 1.0230 - accuracy: 0.7699\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "119/136 [=========================>....] - ETA: 58s - loss: 1.0244 - accuracy: 0.7694 \n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "120/136 [=========================>....] - ETA: 55s - loss: 1.0258 - accuracy: 0.7689\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "121/136 [=========================>....] - ETA: 51s - loss: 1.0271 - accuracy: 0.7684\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "122/136 [=========================>....] - ETA: 48s - loss: 1.0283 - accuracy: 0.7680\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "123/136 [==========================>...] - ETA: 44s - loss: 1.0295 - accuracy: 0.7675\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "124/136 [==========================>...] - ETA: 41s - loss: 1.0307 - accuracy: 0.7671\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "125/136 [==========================>...] - ETA: 37s - loss: 1.0318 - accuracy: 0.7666\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "126/136 [==========================>...] - ETA: 34s - loss: 1.0328 - accuracy: 0.7662\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "127/136 [===========================>..] - ETA: 31s - loss: 1.0339 - accuracy: 0.7658\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "128/136 [===========================>..] - ETA: 27s - loss: 1.0348 - accuracy: 0.7654\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "129/136 [===========================>..] - ETA: 24s - loss: 1.0357 - accuracy: 0.7650\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "130/136 [===========================>..] - ETA: 20s - loss: 1.0366 - accuracy: 0.7646\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "131/136 [===========================>..] - ETA: 17s - loss: 1.0374 - accuracy: 0.7642\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "132/136 [============================>.] - ETA: 13s - loss: 1.0382 - accuracy: 0.7638\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "133/136 [============================>.] - ETA: 10s - loss: 1.0389 - accuracy: 0.7635\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "134/136 [============================>.] - ETA: 6s - loss: 1.0397 - accuracy: 0.7631 \n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "135/136 [============================>.] - ETA: 3s - loss: 1.0405 - accuracy: 0.7628\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "136/136 [==============================] - ETA: 0s - loss: 1.0412 - accuracy: 0.7625\n",
            "Epoch 00008: loss did not improve from 0.32674\n",
            "136/136 [==============================] - 516s 4s/step - loss: 1.0420 - accuracy: 0.7621 - val_loss: 0.9164 - val_accuracy: 0.7667\n",
            "Epoch 9/10\n",
            "  1/136 [..............................] - ETA: 12:09 - loss: 0.8384 - accuracy: 0.7000\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "  2/136 [..............................] - ETA: 8:28 - loss: 0.8462 - accuracy: 0.6750 \n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "  3/136 [..............................] - ETA: 8:37 - loss: 0.9233 - accuracy: 0.6278\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "  4/136 [..............................] - ETA: 8:31 - loss: 0.9411 - accuracy: 0.6146\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "  5/136 [>.............................] - ETA: 8:13 - loss: 0.9514 - accuracy: 0.6157\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "  6/136 [>.............................] - ETA: 8:02 - loss: 0.9465 - accuracy: 0.6186\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "  7/136 [>.............................] - ETA: 7:52 - loss: 0.9615 - accuracy: 0.6139\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "  8/136 [>.............................] - ETA: 7:46 - loss: 0.9638 - accuracy: 0.6153\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "  9/136 [>.............................] - ETA: 7:40 - loss: 0.9683 - accuracy: 0.6148\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 10/136 [=>............................] - ETA: 7:32 - loss: 0.9681 - accuracy: 0.6143\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 11/136 [=>............................] - ETA: 7:32 - loss: 0.9632 - accuracy: 0.6164\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 12/136 [=>............................] - ETA: 7:29 - loss: 0.9572 - accuracy: 0.6178\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 13/136 [=>............................] - ETA: 7:23 - loss: 0.9494 - accuracy: 0.6199\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 14/136 [==>...........................] - ETA: 7:17 - loss: 0.9420 - accuracy: 0.6221\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 15/136 [==>...........................] - ETA: 7:10 - loss: 0.9344 - accuracy: 0.6251\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 16/136 [==>...........................] - ETA: 7:03 - loss: 0.9258 - accuracy: 0.6286\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 17/136 [==>...........................] - ETA: 6:59 - loss: 0.9175 - accuracy: 0.6321\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 18/136 [==>...........................] - ETA: 6:55 - loss: 0.9111 - accuracy: 0.6356\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 19/136 [===>..........................] - ETA: 6:52 - loss: 0.9053 - accuracy: 0.6387\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 20/136 [===>..........................] - ETA: 6:48 - loss: 0.8995 - accuracy: 0.6420\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 21/136 [===>..........................] - ETA: 6:43 - loss: 0.8945 - accuracy: 0.6450\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 22/136 [===>..........................] - ETA: 6:39 - loss: 0.8907 - accuracy: 0.6475\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 23/136 [====>.........................] - ETA: 6:35 - loss: 0.8877 - accuracy: 0.6499\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 24/136 [====>.........................] - ETA: 6:31 - loss: 0.8865 - accuracy: 0.6522\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 25/136 [====>.........................] - ETA: 6:27 - loss: 0.8856 - accuracy: 0.6543\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 26/136 [====>.........................] - ETA: 6:23 - loss: 0.8849 - accuracy: 0.6560\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 27/136 [====>.........................] - ETA: 6:20 - loss: 0.8839 - accuracy: 0.6579\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 28/136 [=====>........................] - ETA: 6:17 - loss: 0.8824 - accuracy: 0.6598\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 29/136 [=====>........................] - ETA: 6:13 - loss: 0.8812 - accuracy: 0.6614\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 30/136 [=====>........................] - ETA: 6:09 - loss: 0.8796 - accuracy: 0.6632\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 31/136 [=====>........................] - ETA: 6:05 - loss: 0.8779 - accuracy: 0.6649\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 32/136 [======>.......................] - ETA: 6:02 - loss: 0.8764 - accuracy: 0.6665\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 33/136 [======>.......................] - ETA: 5:58 - loss: 0.8749 - accuracy: 0.6680\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 34/136 [======>.......................] - ETA: 5:54 - loss: 0.8729 - accuracy: 0.6698\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 35/136 [======>.......................] - ETA: 5:50 - loss: 0.8710 - accuracy: 0.6714\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 36/136 [======>.......................] - ETA: 5:47 - loss: 0.8692 - accuracy: 0.6729\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 37/136 [=======>......................] - ETA: 5:43 - loss: 0.8673 - accuracy: 0.6745\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 38/136 [=======>......................] - ETA: 5:40 - loss: 0.8655 - accuracy: 0.6759\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 39/136 [=======>......................] - ETA: 5:36 - loss: 0.8652 - accuracy: 0.6772\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 40/136 [=======>......................] - ETA: 5:33 - loss: 0.8648 - accuracy: 0.6784\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 41/136 [========>.....................] - ETA: 5:29 - loss: 0.8643 - accuracy: 0.6796\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 42/136 [========>.....................] - ETA: 5:26 - loss: 0.8636 - accuracy: 0.6808\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 43/136 [========>.....................] - ETA: 5:22 - loss: 0.8633 - accuracy: 0.6818\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 44/136 [========>.....................] - ETA: 5:18 - loss: 0.8628 - accuracy: 0.6829\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 45/136 [========>.....................] - ETA: 5:14 - loss: 0.8621 - accuracy: 0.6841\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 46/136 [=========>....................] - ETA: 5:10 - loss: 0.8613 - accuracy: 0.6852\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 47/136 [=========>....................] - ETA: 5:07 - loss: 0.8606 - accuracy: 0.6863\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 48/136 [=========>....................] - ETA: 5:04 - loss: 0.8598 - accuracy: 0.6875\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 49/136 [=========>....................] - ETA: 5:00 - loss: 0.8595 - accuracy: 0.6884\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 50/136 [==========>...................] - ETA: 4:56 - loss: 0.8592 - accuracy: 0.6893\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 51/136 [==========>...................] - ETA: 4:53 - loss: 0.8591 - accuracy: 0.6900\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 52/136 [==========>...................] - ETA: 4:49 - loss: 0.8594 - accuracy: 0.6906\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 53/136 [==========>...................] - ETA: 4:46 - loss: 0.8596 - accuracy: 0.6911\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 54/136 [==========>...................] - ETA: 4:42 - loss: 0.8600 - accuracy: 0.6916\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 55/136 [===========>..................] - ETA: 4:39 - loss: 0.8605 - accuracy: 0.6921\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 56/136 [===========>..................] - ETA: 4:35 - loss: 0.8610 - accuracy: 0.6924\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 57/136 [===========>..................] - ETA: 4:32 - loss: 0.8616 - accuracy: 0.6927\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 58/136 [===========>..................] - ETA: 4:28 - loss: 0.8621 - accuracy: 0.6930\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 59/136 [============>.................] - ETA: 4:24 - loss: 0.8628 - accuracy: 0.6932\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 60/136 [============>.................] - ETA: 4:21 - loss: 0.8635 - accuracy: 0.6933\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 61/136 [============>.................] - ETA: 4:17 - loss: 0.8642 - accuracy: 0.6934\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 62/136 [============>.................] - ETA: 4:14 - loss: 0.8648 - accuracy: 0.6936\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 63/136 [============>.................] - ETA: 4:10 - loss: 0.8654 - accuracy: 0.6937\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 64/136 [=============>................] - ETA: 4:07 - loss: 0.8658 - accuracy: 0.6940\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 65/136 [=============>................] - ETA: 4:03 - loss: 0.8662 - accuracy: 0.6942\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 66/136 [=============>................] - ETA: 4:01 - loss: 0.8665 - accuracy: 0.6944\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 67/136 [=============>................] - ETA: 4:01 - loss: 0.8669 - accuracy: 0.6946\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 68/136 [==============>...............] - ETA: 3:58 - loss: 0.8671 - accuracy: 0.6947\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 69/136 [==============>...............] - ETA: 3:54 - loss: 0.8675 - accuracy: 0.6949\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 70/136 [==============>...............] - ETA: 3:51 - loss: 0.8678 - accuracy: 0.6951\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 71/136 [==============>...............] - ETA: 3:47 - loss: 0.8680 - accuracy: 0.6953\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 72/136 [==============>...............] - ETA: 3:44 - loss: 0.8682 - accuracy: 0.6955\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 73/136 [===============>..............] - ETA: 3:40 - loss: 0.8685 - accuracy: 0.6956\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 74/136 [===============>..............] - ETA: 3:37 - loss: 0.8687 - accuracy: 0.6958\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 75/136 [===============>..............] - ETA: 3:33 - loss: 0.8689 - accuracy: 0.6959\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 76/136 [===============>..............] - ETA: 3:30 - loss: 0.8691 - accuracy: 0.6961\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 77/136 [===============>..............] - ETA: 3:26 - loss: 0.8691 - accuracy: 0.6963\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 78/136 [================>.............] - ETA: 3:22 - loss: 0.8692 - accuracy: 0.6965\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 79/136 [================>.............] - ETA: 3:18 - loss: 0.8694 - accuracy: 0.6967\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 80/136 [================>.............] - ETA: 3:15 - loss: 0.8694 - accuracy: 0.6969\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 81/136 [================>.............] - ETA: 3:10 - loss: 0.8694 - accuracy: 0.6971\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 82/136 [=================>............] - ETA: 3:07 - loss: 0.8694 - accuracy: 0.6973\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 83/136 [=================>............] - ETA: 3:03 - loss: 0.8692 - accuracy: 0.6975\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 84/136 [=================>............] - ETA: 3:00 - loss: 0.8690 - accuracy: 0.6978\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 85/136 [=================>............] - ETA: 2:56 - loss: 0.8688 - accuracy: 0.6981\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 86/136 [=================>............] - ETA: 2:53 - loss: 0.8686 - accuracy: 0.6984\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 87/136 [==================>...........] - ETA: 2:49 - loss: 0.8683 - accuracy: 0.6987\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 88/136 [==================>...........] - ETA: 2:46 - loss: 0.8680 - accuracy: 0.6990\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 89/136 [==================>...........] - ETA: 2:42 - loss: 0.8676 - accuracy: 0.6993\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 90/136 [==================>...........] - ETA: 2:39 - loss: 0.8672 - accuracy: 0.6996\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 91/136 [===================>..........] - ETA: 2:36 - loss: 0.8674 - accuracy: 0.7000\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 92/136 [===================>..........] - ETA: 2:32 - loss: 0.8677 - accuracy: 0.7004\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 93/136 [===================>..........] - ETA: 2:29 - loss: 0.8679 - accuracy: 0.7007\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 94/136 [===================>..........] - ETA: 2:25 - loss: 0.8681 - accuracy: 0.7010\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 95/136 [===================>..........] - ETA: 2:22 - loss: 0.8685 - accuracy: 0.7013\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 96/136 [====================>.........] - ETA: 2:18 - loss: 0.8688 - accuracy: 0.7016\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 97/136 [====================>.........] - ETA: 2:15 - loss: 0.8691 - accuracy: 0.7019\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 98/136 [====================>.........] - ETA: 2:11 - loss: 0.8751 - accuracy: 0.7021\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            " 99/136 [====================>.........] - ETA: 2:08 - loss: 0.8809 - accuracy: 0.7023\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "100/136 [=====================>........] - ETA: 2:04 - loss: 0.8866 - accuracy: 0.7026\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "101/136 [=====================>........] - ETA: 2:01 - loss: 0.8921 - accuracy: 0.7028\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "102/136 [=====================>........] - ETA: 1:57 - loss: 0.8974 - accuracy: 0.7030\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "103/136 [=====================>........] - ETA: 1:54 - loss: 0.9025 - accuracy: 0.7032\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "104/136 [=====================>........] - ETA: 1:50 - loss: 0.9075 - accuracy: 0.7033\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "105/136 [======================>.......] - ETA: 1:47 - loss: 0.9124 - accuracy: 0.7035\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "106/136 [======================>.......] - ETA: 1:43 - loss: 0.9170 - accuracy: 0.7037\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "107/136 [======================>.......] - ETA: 1:40 - loss: 0.9216 - accuracy: 0.7039\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "108/136 [======================>.......] - ETA: 1:36 - loss: 0.9260 - accuracy: 0.7041\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "109/136 [=======================>......] - ETA: 1:33 - loss: 0.9304 - accuracy: 0.7042\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "110/136 [=======================>......] - ETA: 1:29 - loss: 0.9346 - accuracy: 0.7043\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "111/136 [=======================>......] - ETA: 1:26 - loss: 0.9388 - accuracy: 0.7045\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "112/136 [=======================>......] - ETA: 1:23 - loss: 0.9428 - accuracy: 0.7046\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "113/136 [=======================>......] - ETA: 1:19 - loss: 0.9467 - accuracy: 0.7047\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "114/136 [========================>.....] - ETA: 1:16 - loss: 0.9505 - accuracy: 0.7049\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "115/136 [========================>.....] - ETA: 1:12 - loss: 0.9541 - accuracy: 0.7050\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "116/136 [========================>.....] - ETA: 1:09 - loss: 0.9576 - accuracy: 0.7052\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "117/136 [========================>.....] - ETA: 1:05 - loss: 0.9609 - accuracy: 0.7054\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "118/136 [=========================>....] - ETA: 1:02 - loss: 0.9642 - accuracy: 0.7056\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "119/136 [=========================>....] - ETA: 58s - loss: 0.9673 - accuracy: 0.7057 \n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "120/136 [=========================>....] - ETA: 55s - loss: 0.9703 - accuracy: 0.7059\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "121/136 [=========================>....] - ETA: 51s - loss: 0.9733 - accuracy: 0.7061\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "122/136 [=========================>....] - ETA: 48s - loss: 0.9761 - accuracy: 0.7063\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "123/136 [==========================>...] - ETA: 44s - loss: 0.9789 - accuracy: 0.7065\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "124/136 [==========================>...] - ETA: 41s - loss: 0.9815 - accuracy: 0.7067\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "125/136 [==========================>...] - ETA: 38s - loss: 0.9841 - accuracy: 0.7069\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "126/136 [==========================>...] - ETA: 34s - loss: 0.9867 - accuracy: 0.7071\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "127/136 [===========================>..] - ETA: 31s - loss: 0.9891 - accuracy: 0.7073\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "128/136 [===========================>..] - ETA: 27s - loss: 0.9915 - accuracy: 0.7075\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "129/136 [===========================>..] - ETA: 24s - loss: 0.9939 - accuracy: 0.7077\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "130/136 [===========================>..] - ETA: 20s - loss: 0.9962 - accuracy: 0.7079\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "131/136 [===========================>..] - ETA: 17s - loss: 0.9984 - accuracy: 0.7081\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "132/136 [============================>.] - ETA: 13s - loss: 1.0005 - accuracy: 0.7082\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "133/136 [============================>.] - ETA: 10s - loss: 1.0026 - accuracy: 0.7084\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "134/136 [============================>.] - ETA: 6s - loss: 1.0047 - accuracy: 0.7086 \n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "135/136 [============================>.] - ETA: 3s - loss: 1.0067 - accuracy: 0.7088\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "136/136 [==============================] - ETA: 0s - loss: 1.0086 - accuracy: 0.7089\n",
            "Epoch 00009: loss did not improve from 0.32674\n",
            "136/136 [==============================] - 516s 4s/step - loss: 1.0106 - accuracy: 0.7091 - val_loss: 1.0261 - val_accuracy: 0.7375\n",
            "Epoch 10/10\n",
            "  1/136 [..............................] - ETA: 11:14 - loss: 0.3342 - accuracy: 1.0000\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "  2/136 [..............................] - ETA: 8:42 - loss: 0.5769 - accuracy: 0.9000 \n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "  3/136 [..............................] - ETA: 8:37 - loss: 0.6564 - accuracy: 0.8667\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "  4/136 [..............................] - ETA: 8:20 - loss: 0.6961 - accuracy: 0.8500\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "  5/136 [>.............................] - ETA: 8:03 - loss: 0.7101 - accuracy: 0.8400\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "  6/136 [>.............................] - ETA: 7:54 - loss: 0.7117 - accuracy: 0.8361\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "  7/136 [>.............................] - ETA: 7:46 - loss: 0.7126 - accuracy: 0.8310\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "  8/136 [>.............................] - ETA: 7:38 - loss: 0.7112 - accuracy: 0.8271\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "  9/136 [>.............................] - ETA: 7:32 - loss: 0.7098 - accuracy: 0.8241\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 10/136 [=>............................] - ETA: 7:26 - loss: 0.7090 - accuracy: 0.8217\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 11/136 [=>............................] - ETA: 7:19 - loss: 0.7067 - accuracy: 0.8197\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 12/136 [=>............................] - ETA: 7:14 - loss: 0.7079 - accuracy: 0.8160\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 13/136 [=>............................] - ETA: 7:08 - loss: 0.7069 - accuracy: 0.8136\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 14/136 [==>...........................] - ETA: 7:04 - loss: 0.7071 - accuracy: 0.8111\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 15/136 [==>...........................] - ETA: 7:00 - loss: 0.7057 - accuracy: 0.8099\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 16/136 [==>...........................] - ETA: 7:03 - loss: 0.7054 - accuracy: 0.8081\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 17/136 [==>...........................] - ETA: 6:59 - loss: 0.7039 - accuracy: 0.8069\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 18/136 [==>...........................] - ETA: 6:53 - loss: 0.7037 - accuracy: 0.8056\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 19/136 [===>..........................] - ETA: 6:48 - loss: 0.7042 - accuracy: 0.8042\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 20/136 [===>..........................] - ETA: 6:45 - loss: 0.7049 - accuracy: 0.8027\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 21/136 [===>..........................] - ETA: 6:41 - loss: 0.7051 - accuracy: 0.8015\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 22/136 [===>..........................] - ETA: 6:38 - loss: 0.7043 - accuracy: 0.8008\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 23/136 [====>.........................] - ETA: 6:34 - loss: 0.7047 - accuracy: 0.7998\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 24/136 [====>.........................] - ETA: 6:30 - loss: 0.7058 - accuracy: 0.7984\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 25/136 [====>.........................] - ETA: 6:27 - loss: 0.7070 - accuracy: 0.7971\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 26/136 [====>.........................] - ETA: 6:23 - loss: 0.7074 - accuracy: 0.7958\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 27/136 [====>.........................] - ETA: 6:18 - loss: 0.7076 - accuracy: 0.7949\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 28/136 [=====>........................] - ETA: 6:14 - loss: 0.7075 - accuracy: 0.7942\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 29/136 [=====>........................] - ETA: 6:12 - loss: 0.7075 - accuracy: 0.7934\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 30/136 [=====>........................] - ETA: 6:08 - loss: 0.7072 - accuracy: 0.7929\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 31/136 [=====>........................] - ETA: 6:05 - loss: 0.7066 - accuracy: 0.7925\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 32/136 [======>.......................] - ETA: 6:01 - loss: 0.7055 - accuracy: 0.7923\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 33/136 [======>.......................] - ETA: 5:57 - loss: 0.7042 - accuracy: 0.7923\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 34/136 [======>.......................] - ETA: 5:54 - loss: 0.7026 - accuracy: 0.7923\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 35/136 [======>.......................] - ETA: 5:50 - loss: 0.7011 - accuracy: 0.7925\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 36/136 [======>.......................] - ETA: 5:47 - loss: 0.6993 - accuracy: 0.7928\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 37/136 [=======>......................] - ETA: 5:43 - loss: 0.6975 - accuracy: 0.7931\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 38/136 [=======>......................] - ETA: 5:39 - loss: 0.6960 - accuracy: 0.7933\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 39/136 [=======>......................] - ETA: 5:36 - loss: 0.6945 - accuracy: 0.7935\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 40/136 [=======>......................] - ETA: 5:33 - loss: 0.6930 - accuracy: 0.7937\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 41/136 [========>.....................] - ETA: 5:29 - loss: 0.6916 - accuracy: 0.7939\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 42/136 [========>.....................] - ETA: 5:25 - loss: 0.6901 - accuracy: 0.7940\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 43/136 [========>.....................] - ETA: 5:22 - loss: 0.6883 - accuracy: 0.7943\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 44/136 [========>.....................] - ETA: 5:19 - loss: 0.6867 - accuracy: 0.7945\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 45/136 [========>.....................] - ETA: 5:15 - loss: 0.6850 - accuracy: 0.7947\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 46/136 [=========>....................] - ETA: 5:11 - loss: 0.6834 - accuracy: 0.7950\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 47/136 [=========>....................] - ETA: 5:07 - loss: 0.6817 - accuracy: 0.7953\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 48/136 [=========>....................] - ETA: 5:04 - loss: 0.6801 - accuracy: 0.7956\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 49/136 [=========>....................] - ETA: 5:00 - loss: 0.6784 - accuracy: 0.7959\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 50/136 [==========>...................] - ETA: 4:56 - loss: 0.6769 - accuracy: 0.7961\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 51/136 [==========>...................] - ETA: 4:53 - loss: 0.6755 - accuracy: 0.7963\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 52/136 [==========>...................] - ETA: 4:49 - loss: 0.6740 - accuracy: 0.7966\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 53/136 [==========>...................] - ETA: 4:46 - loss: 0.6727 - accuracy: 0.7968\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 54/136 [==========>...................] - ETA: 4:43 - loss: 0.6714 - accuracy: 0.7970\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 55/136 [===========>..................] - ETA: 4:39 - loss: 0.6701 - accuracy: 0.7973\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 56/136 [===========>..................] - ETA: 4:37 - loss: 0.6688 - accuracy: 0.7975\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 57/136 [===========>..................] - ETA: 4:33 - loss: 0.6678 - accuracy: 0.7976\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 58/136 [===========>..................] - ETA: 4:31 - loss: 0.6669 - accuracy: 0.7977\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 59/136 [============>.................] - ETA: 4:27 - loss: 0.6659 - accuracy: 0.7978\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 60/136 [============>.................] - ETA: 4:24 - loss: 0.6649 - accuracy: 0.7979\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 61/136 [============>.................] - ETA: 4:20 - loss: 0.6640 - accuracy: 0.7980\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 62/136 [============>.................] - ETA: 4:17 - loss: 0.6631 - accuracy: 0.7981\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 63/136 [============>.................] - ETA: 4:13 - loss: 0.6622 - accuracy: 0.7982\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 64/136 [=============>................] - ETA: 4:09 - loss: 0.6615 - accuracy: 0.7983\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 65/136 [=============>................] - ETA: 4:06 - loss: 0.6607 - accuracy: 0.7984\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 66/136 [=============>................] - ETA: 4:02 - loss: 0.6600 - accuracy: 0.7985\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 67/136 [=============>................] - ETA: 3:59 - loss: 0.6594 - accuracy: 0.7985\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 68/136 [==============>...............] - ETA: 3:55 - loss: 0.6588 - accuracy: 0.7986\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 69/136 [==============>...............] - ETA: 3:52 - loss: 0.6581 - accuracy: 0.7987\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 70/136 [==============>...............] - ETA: 3:48 - loss: 0.6575 - accuracy: 0.7988\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 71/136 [==============>...............] - ETA: 3:44 - loss: 0.6570 - accuracy: 0.7988\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 72/136 [==============>...............] - ETA: 3:41 - loss: 0.6565 - accuracy: 0.7988\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 73/136 [===============>..............] - ETA: 3:37 - loss: 0.6561 - accuracy: 0.7989\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 74/136 [===============>..............] - ETA: 3:34 - loss: 0.6558 - accuracy: 0.7989\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 75/136 [===============>..............] - ETA: 3:30 - loss: 0.6553 - accuracy: 0.7989\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 76/136 [===============>..............] - ETA: 3:27 - loss: 0.6550 - accuracy: 0.7989\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 77/136 [===============>..............] - ETA: 3:23 - loss: 0.6547 - accuracy: 0.7989\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 78/136 [================>.............] - ETA: 3:20 - loss: 0.6544 - accuracy: 0.7989\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 79/136 [================>.............] - ETA: 3:16 - loss: 0.6541 - accuracy: 0.7989\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 80/136 [================>.............] - ETA: 3:13 - loss: 0.6538 - accuracy: 0.7989\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 81/136 [================>.............] - ETA: 3:13 - loss: 0.6535 - accuracy: 0.7989\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 82/136 [=================>............] - ETA: 3:09 - loss: 0.6534 - accuracy: 0.7989\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 83/136 [=================>............] - ETA: 3:05 - loss: 0.6532 - accuracy: 0.7988\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 84/136 [=================>............] - ETA: 3:02 - loss: 0.6530 - accuracy: 0.7988\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 85/136 [=================>............] - ETA: 2:58 - loss: 0.6528 - accuracy: 0.7988\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 86/136 [=================>............] - ETA: 2:55 - loss: 0.6527 - accuracy: 0.7987\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 87/136 [==================>...........] - ETA: 2:51 - loss: 0.6526 - accuracy: 0.7987\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 88/136 [==================>...........] - ETA: 2:48 - loss: 0.6525 - accuracy: 0.7987\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 89/136 [==================>...........] - ETA: 2:44 - loss: 0.6525 - accuracy: 0.7986\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 90/136 [==================>...........] - ETA: 2:41 - loss: 0.6525 - accuracy: 0.7984\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 91/136 [===================>..........] - ETA: 2:37 - loss: 0.6525 - accuracy: 0.7983\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 92/136 [===================>..........] - ETA: 2:34 - loss: 0.6525 - accuracy: 0.7982\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 93/136 [===================>..........] - ETA: 2:30 - loss: 0.6525 - accuracy: 0.7981\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 94/136 [===================>..........] - ETA: 2:26 - loss: 0.6524 - accuracy: 0.7980\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 95/136 [===================>..........] - ETA: 2:23 - loss: 0.6523 - accuracy: 0.7979\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 96/136 [====================>.........] - ETA: 2:19 - loss: 0.6523 - accuracy: 0.7978\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 97/136 [====================>.........] - ETA: 2:16 - loss: 0.6522 - accuracy: 0.7977\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 98/136 [====================>.........] - ETA: 2:12 - loss: 0.6521 - accuracy: 0.7977\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            " 99/136 [====================>.........] - ETA: 2:09 - loss: 0.6521 - accuracy: 0.7976\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "100/136 [=====================>........] - ETA: 2:05 - loss: 0.6521 - accuracy: 0.7975\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "101/136 [=====================>........] - ETA: 2:02 - loss: 0.6520 - accuracy: 0.7974\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "102/136 [=====================>........] - ETA: 1:58 - loss: 0.6520 - accuracy: 0.7973\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "103/136 [=====================>........] - ETA: 1:55 - loss: 0.6519 - accuracy: 0.7973\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "104/136 [=====================>........] - ETA: 1:51 - loss: 0.6519 - accuracy: 0.7972\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "105/136 [======================>.......] - ETA: 1:48 - loss: 0.6518 - accuracy: 0.7972\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "106/136 [======================>.......] - ETA: 1:44 - loss: 0.6518 - accuracy: 0.7971\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "107/136 [======================>.......] - ETA: 1:41 - loss: 0.6518 - accuracy: 0.7971\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "108/136 [======================>.......] - ETA: 1:37 - loss: 0.6518 - accuracy: 0.7970\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "109/136 [=======================>......] - ETA: 1:34 - loss: 0.6518 - accuracy: 0.7969\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "110/136 [=======================>......] - ETA: 1:30 - loss: 0.6518 - accuracy: 0.7969\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "111/136 [=======================>......] - ETA: 1:27 - loss: 0.6518 - accuracy: 0.7968\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "112/136 [=======================>......] - ETA: 1:23 - loss: 0.6518 - accuracy: 0.7967\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "113/136 [=======================>......] - ETA: 1:20 - loss: 0.6519 - accuracy: 0.7966\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "114/136 [========================>.....] - ETA: 1:16 - loss: 0.6519 - accuracy: 0.7965\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "115/136 [========================>.....] - ETA: 1:13 - loss: 0.6519 - accuracy: 0.7965\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "116/136 [========================>.....] - ETA: 1:09 - loss: 0.6519 - accuracy: 0.7964\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "117/136 [========================>.....] - ETA: 1:06 - loss: 0.6520 - accuracy: 0.7964\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "118/136 [=========================>....] - ETA: 1:02 - loss: 0.6520 - accuracy: 0.7963\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "119/136 [=========================>....] - ETA: 59s - loss: 0.6520 - accuracy: 0.7962 \n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "120/136 [=========================>....] - ETA: 55s - loss: 0.6520 - accuracy: 0.7962\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "121/136 [=========================>....] - ETA: 52s - loss: 0.6520 - accuracy: 0.7961\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "122/136 [=========================>....] - ETA: 48s - loss: 0.6520 - accuracy: 0.7961\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "123/136 [==========================>...] - ETA: 45s - loss: 0.6520 - accuracy: 0.7960\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "124/136 [==========================>...] - ETA: 41s - loss: 0.6520 - accuracy: 0.7960\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "125/136 [==========================>...] - ETA: 38s - loss: 0.6521 - accuracy: 0.7959\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "126/136 [==========================>...] - ETA: 34s - loss: 0.6521 - accuracy: 0.7959\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "127/136 [===========================>..] - ETA: 31s - loss: 0.6522 - accuracy: 0.7958\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "128/136 [===========================>..] - ETA: 27s - loss: 0.6524 - accuracy: 0.7958\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "129/136 [===========================>..] - ETA: 24s - loss: 0.6525 - accuracy: 0.7957\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "130/136 [===========================>..] - ETA: 20s - loss: 0.6527 - accuracy: 0.7956\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "131/136 [===========================>..] - ETA: 17s - loss: 0.6529 - accuracy: 0.7956\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "132/136 [============================>.] - ETA: 13s - loss: 0.6531 - accuracy: 0.7955\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "133/136 [============================>.] - ETA: 10s - loss: 0.6534 - accuracy: 0.7954\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "134/136 [============================>.] - ETA: 6s - loss: 0.6536 - accuracy: 0.7953 \n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "135/136 [============================>.] - ETA: 3s - loss: 0.6539 - accuracy: 0.7952\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "136/136 [==============================] - ETA: 0s - loss: 0.6541 - accuracy: 0.7951\n",
            "Epoch 00010: loss did not improve from 0.32674\n",
            "136/136 [==============================] - 516s 4s/step - loss: 0.6544 - accuracy: 0.7950 - val_loss: 1.0887 - val_accuracy: 0.7083\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}